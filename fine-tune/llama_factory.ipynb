{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# https://github.com/hiyouga/LLaMA-Factory"
      ],
      "metadata": {
        "id": "fC6_5nn7t7oN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "pYNCoR42gT0I"
      },
      "outputs": [],
      "source": [
        "# !git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "# %cd LLaMA-Factory\n",
        "# !pip install -r -q requirements.txt\n",
        "# !pip install -q bitsandbytes>=0.39.0\n",
        "# !pip install -q einops"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pJtjGSbkmVqm",
        "outputId": "5f7e7d22-9b5d-4daf-e4dc-917188ff8340"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[?25l     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/44.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/train_web.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-PDkQzfok8uU",
        "outputId": "fcb12626-0a9f-437b-d754-7dbb3b9f71a9"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-21 20:39:08.985650: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-21 20:39:08.985703: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-21 20:39:08.985741: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-21 20:39:10.241260: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://d3f5150774e76a86ac.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "11/21/2023 20:41:23 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:41:23,574 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:41:23,574 >> PyTorch: setting up devices\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1711: FutureWarning: `--push_to_hub_token` is deprecated and will be removed in version 5 of ü§ó Transformers. Use `--hub_token` instead.\n",
            "  warnings.warn(\n",
            "11/21/2023 20:41:23 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "11/21/2023 20:41:23 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=4,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saves/Phi1.5-1.3B/lora/2023-11-21-20-39-38/runs/Nov21_20-41-23_63b98ab43392,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=saves/Phi1.5-1.3B/lora/2023-11-21-20-39-38,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=4,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saves/Phi1.5-1.3B/lora/2023-11-21-20-39-38,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=100,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/21/2023 20:41:23 - INFO - llmtuner.data.loader - Loading dataset lima.json...\n",
            "Using custom data configuration default-712a4dc361df6b3c\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:41:24,384 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:41:24,384 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:41:24,384 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:41:24,385 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:41:24,385 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:41:24,385 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-21 20:41:24,603 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-21 20:41:24,833 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-11-21 20:41:24,834 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
            "  },\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"flash_attn\": false,\n",
            "  \"flash_rotary\": false,\n",
            "  \"fused_dense\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"n_embd\": 2048,\n",
            "  \"n_head\": 32,\n",
            "  \"n_head_kv\": null,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 2048,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rotary_dim\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "11/21/2023 20:41:24 - INFO - llmtuner.model.loader - Quantizing model to 4 bit.\n",
            "Downloading pytorch_model.bin: 100% 2.84G/2.84G [00:14<00:00, 198MB/s]\n",
            "[INFO|modeling_utils.py:2993] 2023-11-21 20:41:39,801 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1220] 2023-11-21 20:41:42,009 >> Instantiating PhiForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:770] 2023-11-21 20:41:42,226 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|configuration_utils.py:770] 2023-11-21 20:41:42,228 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|modeling_utils.py:3103] 2023-11-21 20:41:42,482 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
            "[INFO|modeling_utils.py:3775] 2023-11-21 20:41:53,384 >> All model checkpoint weights were used when initializing PhiForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:3783] 2023-11-21 20:41:53,384 >> All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-1_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
            "Downloading generation_config.json: 100% 69.0/69.0 [00:00<00:00, 369kB/s]\n",
            "[INFO|configuration_utils.py:730] 2023-11-21 20:41:53,608 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/generation_config.json\n",
            "[INFO|configuration_utils.py:770] 2023-11-21 20:41:53,608 >> Generate config GenerationConfig {}\n",
            "\n",
            "11/21/2023 20:41:53 - INFO - llmtuner.model.utils - Upcasting weights in layernorm in float32.\n",
            "11/21/2023 20:41:53 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "11/21/2023 20:41:53 - INFO - llmtuner.model.loader - trainable params: 1572864 || all params: 1419843584 || trainable%: 0.1108\n",
            "11/21/2023 20:41:53 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "Running tokenizer on dataset:   0% 0/1029 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3823] 2023-11-21 20:41:54,799 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3639 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-bd9c7f7ba3c88201.arrow\n",
            "Running tokenizer on dataset: 100% 1029/1029 [00:02<00:00, 350.68 examples/s]\n",
            "input_ids:\n",
            "[50256, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 628, 21017, 46486, 25, 198, 6090, 3632, 4778, 1445, 30, 2750, 3356, 314, 1612, 890, 5253, 13472, 357, 3866, 2232, 1346, 1626, 262, 3632, 691, 737, 198, 198, 21017, 18261, 25, 198, 464, 1808, 318, 5365, 3154, 290, 530, 815, 1011, 656, 1848, 326, 262, 3632, 407, 691, 10874, 286, 16890, 11, 475, 635, 1278, 498, 4778, 357, 11284, 425, 4778, 8, 290, 662, 12, 2781, 6210, 36347, 10717, 4778, 13, 11399, 11, 355, 4688, 5891, 12, 25346, 1023, 423, 8203, 11, 23456, 3800, 318, 845, 1593, 11, 355, 262, 5922, 49710, 3632, 318, 845, 1180, 422, 262, 4044, 3632, 13, 198, 4864, 11, 706, 264, 13309, 832, 2972, 16125, 11, 262, 3280, 284, 262, 1808, 318, 1682, 21196, 2829, 25, 3363, 11, 3632, 4778, 32492, 13, 198, 818, 50286, 1169, 4044, 3632, 1278, 498, 4778, 32492, 287, 262, 3632, 357, 42, 75, 11033, 2022, 83, 11, 3717, 737, 2671, 498, 4778, 389, 2950, 287, 257, 24862, 286, 5499, 11, 475, 257, 12411, 1672, 286, 45879, 1278, 498, 4778, 389, 262, 24869, 375, 437, 305, 948, 4879, 326, 32492, 3585, 890, 18868, 284, 1064, 511, 2496, 7877, 684, 4291, 543, 484, 14441, 2405, 284, 1296, 262, 1035, 8306, 616, 27176, 673, 776, 357, 33758, 1872, 290, 7920, 11, 6244, 737, 198, 8199, 333, 20996, 10717, 4778, 32492, 625, 890, 18868, 287, 2882, 284, 5095, 357, 40, 2781, 5708, 2123, 435, 1539, 5472, 8, 290, 484, 32492, 422, 2176, 10717, 12, 3846, 7064, 357, 68, 13, 70, 1539, 38587, 290, 850, 1151, 41001, 6516, 8, 284, 584, 7652, 357, 48035, 365, 11, 5816, 737, 198, 6307, 12, 2781, 6210, 11, 475, 1729, 12, 39799, 12931, 16890, 423, 587, 3402, 284, 32492, 287, 262, 4044, 3632, 287, 5916, 357, 19040, 2123, 435, 1539, 2321, 828, 290, 287, 23426, 290, 1729, 12, 10734, 41316, 355, 880, 357, 50, 707, 4763, 2123, 435, 1539, 2813, 737, 198, 3673, 12362, 11, 1278, 498, 4778, 11, 10717, 4778, 290, 16890, 635, 32492, 1141, 49710, 2478, 13, 4042, 14660, 11, 1281, 12, 2781, 6210, 16890, 23985, 284, 14658, 25514, 5499, 423, 284, 32492, 625, 5365, 890, 18868, 422, 262, 17019, 36893, 284, 511, 2496, 7064, 357, 8199, 1434, 16801, 11, 362, 358, 1225, 11, 3169, 333, 20996, 36991, 737, 50256]\n",
            "inputs:\n",
            "<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Can brain cells move? By movement I mean long distance migration (preferably within the brain only).\n",
            "\n",
            "### Response:\n",
            "The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
            "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
            "In  the adult brain glial cells migrate in the brain (Kl√§mbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
            "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\n",
            "Post-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\n",
            "Not surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).<|endoftext|>\n",
            "label_ids:\n",
            "[50256, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 628, 21017, 46486, 25, 198, 6090, 3632, 4778, 1445, 30, 2750, 3356, 314, 1612, 890, 5253, 13472, 357, 3866, 2232, 1346, 1626, 262, 3632, 691, 737, 198, 198, 21017, 18261, 25, 198, 464, 1808, 318, 5365, 3154, 290, 530, 815, 1011, 656, 1848, 326, 262, 3632, 407, 691, 10874, 286, 16890, 11, 475, 635, 1278, 498, 4778, 357, 11284, 425, 4778, 8, 290, 662, 12, 2781, 6210, 36347, 10717, 4778, 13, 11399, 11, 355, 4688, 5891, 12, 25346, 1023, 423, 8203, 11, 23456, 3800, 318, 845, 1593, 11, 355, 262, 5922, 49710, 3632, 318, 845, 1180, 422, 262, 4044, 3632, 13, 198, 4864, 11, 706, 264, 13309, 832, 2972, 16125, 11, 262, 3280, 284, 262, 1808, 318, 1682, 21196, 2829, 25, 3363, 11, 3632, 4778, 32492, 13, 198, 818, 50286, 1169, 4044, 3632, 1278, 498, 4778, 32492, 287, 262, 3632, 357, 42, 75, 11033, 2022, 83, 11, 3717, 737, 2671, 498, 4778, 389, 2950, 287, 257, 24862, 286, 5499, 11, 475, 257, 12411, 1672, 286, 45879, 1278, 498, 4778, 389, 262, 24869, 375, 437, 305, 948, 4879, 326, 32492, 3585, 890, 18868, 284, 1064, 511, 2496, 7877, 684, 4291, 543, 484, 14441, 2405, 284, 1296, 262, 1035, 8306, 616, 27176, 673, 776, 357, 33758, 1872, 290, 7920, 11, 6244, 737, 198, 8199, 333, 20996, 10717, 4778, 32492, 625, 890, 18868, 287, 2882, 284, 5095, 357, 40, 2781, 5708, 2123, 435, 1539, 5472, 8, 290, 484, 32492, 422, 2176, 10717, 12, 3846, 7064, 357, 68, 13, 70, 1539, 38587, 290, 850, 1151, 41001, 6516, 8, 284, 584, 7652, 357, 48035, 365, 11, 5816, 737, 198, 6307, 12, 2781, 6210, 11, 475, 1729, 12, 39799, 12931, 16890, 423, 587, 3402, 284, 32492, 287, 262, 4044, 3632, 287, 5916, 357, 19040, 2123, 435, 1539, 2321, 828, 290, 287, 23426, 290, 1729, 12, 10734, 41316, 355, 880, 357, 50, 707, 4763, 2123, 435, 1539, 2813, 737, 198, 3673, 12362, 11, 1278, 498, 4778, 11, 10717, 4778, 290, 16890, 635, 32492, 1141, 49710, 2478, 13, 4042, 14660, 11, 1281, 12, 2781, 6210, 16890, 23985, 284, 14658, 25514, 5499, 423, 284, 32492, 625, 5365, 890, 18868, 422, 262, 17019, 36893, 284, 511, 2496, 7064, 357, 8199, 1434, 16801, 11, 362, 358, 1225, 11, 3169, 333, 20996, 36991, 737, 50256]\n",
            "labels:\n",
            "<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Can brain cells move? By movement I mean long distance migration (preferably within the brain only).\n",
            "\n",
            "### Response:\n",
            "The question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
            "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
            "In  the adult brain glial cells migrate in the brain (Kl√§mbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
            "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone) to other regions (Clarke, 2003).\n",
            "Post-mitotic, but non-differentiated neurons have been shown to migrate in the adult brain in fish (Scott et al., 2012), and in mammals and non-human primates as well (Sawada et al., 2011).\n",
            "Not surprisingly, glial cells, stem cells and neurons also migrate during embryonic development. Most notably, post-mitotic neurons destined to fulfill peripheral functions have to migrate over relatively long distances from the neural crest to their target locations (Neuroscience, 2nd ed, Neuronal Migration).<|endoftext|>\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:41:56,762 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:41:56,762 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:1760] 2023-11-21 20:41:57,170 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2023-11-21 20:41:57,170 >>   Num examples = 1,029\n",
            "[INFO|trainer.py:1762] 2023-11-21 20:41:57,170 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1763] 2023-11-21 20:41:57,170 >>   Instantaneous batch size per device = 4\n",
            "[INFO|trainer.py:1766] 2023-11-21 20:41:57,170 >>   Total train batch size (w. parallel, distributed & accumulation) = 16\n",
            "[INFO|trainer.py:1767] 2023-11-21 20:41:57,170 >>   Gradient Accumulation steps = 4\n",
            "[INFO|trainer.py:1768] 2023-11-21 20:41:57,171 >>   Total optimization steps = 192\n",
            "[INFO|trainer.py:1769] 2023-11-21 20:41:57,173 >>   Number of trainable parameters = 1,572,864\n",
            "[WARNING|logging.py:290] 2023-11-21 20:41:57,190 >> You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "Exception in thread Thread-6 (run_exp):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 26, in run_exp\n",
            "    run_sft(model_args, data_args, training_args, finetuning_args, generating_args, callbacks)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/sft/workflow.py\", line 68, in run_sft\n",
            "    train_result = trainer.train(resume_from_checkpoint=training_args.resume_from_checkpoint)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1591, in train\n",
            "    return inner_training_loop(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1892, in _inner_training_loop\n",
            "    tr_loss_step = self.training_step(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2776, in training_step\n",
            "    loss = self.compute_loss(model, inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2801, in compute_loss\n",
            "    outputs = model(**inputs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 659, in forward\n",
            "    return model_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 647, in __call__\n",
            "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 16, in decorate_autocast\n",
            "    return func(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/peft_model.py\", line 1003, in forward\n",
            "    return self.base_model(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/peft/tuners/tuners_utils.py\", line 107, in forward\n",
            "    return self.model.forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 164, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py\", line 962, in forward\n",
            "    hidden_states = self.transformer(input_ids, past_key_values=past_key_values, attention_mask=attention_mask)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 164, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py\", line 924, in forward\n",
            "    hidden_states = layer(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 164, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py\", line 781, in forward\n",
            "    feed_forward_hidden_states = self.resid_dropout(self.mlp(hidden_states))\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 164, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/root/.cache/huggingface/modules/transformers_modules/microsoft/phi-1_5/5fd430c7bcd28140560faee2014d1228338e19a0/modeling_phi.py\", line 324, in forward\n",
            "    hidden_states = self.fc2(hidden_states)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
            "    return self._call_impl(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
            "    return forward_call(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/hooks.py\", line 164, in new_forward\n",
            "    output = module._old_forward(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py\", line 256, in forward\n",
            "    out = bnb.matmul_4bit(x, self.weight.t(), bias=bias, quant_state=self.weight.quant_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\", line 577, in matmul_4bit\n",
            "    return MatMul4Bit.apply(A, B, out, bias, quant_state)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/function.py\", line 539, in apply\n",
            "    return super().apply(*args, **kwargs)  # type: ignore[misc]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/autograd/_functions.py\", line 516, in forward\n",
            "    output = torch.nn.functional.linear(A, F.dequantize_4bit(B, quant_state).to(A.dtype).t(), bias)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/bitsandbytes/functional.py\", line 1025, in dequantize_4bit\n",
            "    out = torch.empty(quant_state.shape, dtype=quant_state.dtype, device=A.device)\n",
            "torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 32.00 MiB. GPU 0 has a total capacty of 14.75 GiB of which 10.81 MiB is free. Process 161996 has 14.73 GiB memory in use. Of the allocated memory 13.70 GiB is allocated by PyTorch, and 30.15 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:43:16,266 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:43:16,266 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:1519] 2023-11-21 20:43:16,267 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "11/21/2023 20:43:16 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:43:16,269 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:43:16,269 >> PyTorch: setting up devices\n",
            "Exception in thread Thread-10 (run_exp):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n",
            "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/model/parser.py\", line 150, in get_train_args\n",
            "    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
            "ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:45:27,601 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:45:27,601 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:1519] 2023-11-21 20:45:27,601 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "11/21/2023 20:45:27 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:45:27,603 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:45:27,604 >> PyTorch: setting up devices\n",
            "Exception in thread Thread-11 (run_exp):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n",
            "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/model/parser.py\", line 150, in get_train_args\n",
            "    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
            "ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:46:12,785 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:46:12,785 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:1519] 2023-11-21 20:46:12,785 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "11/21/2023 20:46:12 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:46:12,787 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:46:12,787 >> PyTorch: setting up devices\n",
            "Exception in thread Thread-12 (run_exp):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/train/tuner.py\", line 20, in run_exp\n",
            "    model_args, data_args, training_args, finetuning_args, generating_args = get_train_args(args)\n",
            "  File \"/content/LLaMA-Factory/src/llmtuner/model/parser.py\", line 150, in get_train_args\n",
            "    raise ValueError(\"Output directory already exists and is not empty. Please set `overwrite_output_dir`.\")\n",
            "ValueError: Output directory already exists and is not empty. Please set `overwrite_output_dir`.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:46:42,755 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:46:42,755 >> PyTorch: setting up devices\n",
            "[INFO|training_args.py:1519] 2023-11-21 20:46:42,756 >> The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
            "11/21/2023 20:46:42 - WARNING - llmtuner.model.parser - `ddp_find_unused_parameters` needs to be set as False for LoRA in DDP training.\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:46:42,758 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:46:42,758 >> PyTorch: setting up devices\n",
            "11/21/2023 20:46:42 - INFO - llmtuner.model.parser - Process rank: 0, device: cuda:0, n_gpu: 1\n",
            "  distributed training: True, compute dtype: torch.float16\n",
            "11/21/2023 20:46:42 - INFO - llmtuner.model.parser - Training/evaluation parameters Seq2SeqTrainingArguments(\n",
            "_n_gpu=1,\n",
            "adafactor=False,\n",
            "adam_beta1=0.9,\n",
            "adam_beta2=0.999,\n",
            "adam_epsilon=1e-08,\n",
            "auto_find_batch_size=False,\n",
            "bf16=False,\n",
            "bf16_full_eval=False,\n",
            "data_seed=None,\n",
            "dataloader_drop_last=False,\n",
            "dataloader_num_workers=0,\n",
            "dataloader_pin_memory=True,\n",
            "ddp_backend=None,\n",
            "ddp_broadcast_buffers=None,\n",
            "ddp_bucket_cap_mb=None,\n",
            "ddp_find_unused_parameters=False,\n",
            "ddp_timeout=1800,\n",
            "debug=[],\n",
            "deepspeed=None,\n",
            "disable_tqdm=True,\n",
            "dispatch_batches=None,\n",
            "do_eval=False,\n",
            "do_predict=False,\n",
            "do_train=True,\n",
            "eval_accumulation_steps=None,\n",
            "eval_delay=0,\n",
            "eval_steps=None,\n",
            "evaluation_strategy=no,\n",
            "fp16=True,\n",
            "fp16_backend=auto,\n",
            "fp16_full_eval=False,\n",
            "fp16_opt_level=O1,\n",
            "fsdp=[],\n",
            "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_grad_ckpt': False},\n",
            "fsdp_min_num_params=0,\n",
            "fsdp_transformer_layer_cls_to_wrap=None,\n",
            "full_determinism=False,\n",
            "generation_config=None,\n",
            "generation_max_length=None,\n",
            "generation_num_beams=None,\n",
            "gradient_accumulation_steps=1,\n",
            "gradient_checkpointing=False,\n",
            "greater_is_better=None,\n",
            "group_by_length=False,\n",
            "half_precision_backend=auto,\n",
            "hub_always_push=False,\n",
            "hub_model_id=None,\n",
            "hub_private_repo=False,\n",
            "hub_strategy=every_save,\n",
            "hub_token=<HUB_TOKEN>,\n",
            "ignore_data_skip=False,\n",
            "include_inputs_for_metrics=False,\n",
            "include_tokens_per_second=False,\n",
            "jit_mode_eval=False,\n",
            "label_names=None,\n",
            "label_smoothing_factor=0.0,\n",
            "learning_rate=5e-05,\n",
            "length_column_name=length,\n",
            "load_best_model_at_end=False,\n",
            "local_rank=0,\n",
            "log_level=passive,\n",
            "log_level_replica=warning,\n",
            "log_on_each_node=True,\n",
            "logging_dir=saves/Phi1.5-1.3B/lora/sam/runs/Nov21_20-46-42_63b98ab43392,\n",
            "logging_first_step=False,\n",
            "logging_nan_inf_filter=True,\n",
            "logging_steps=5,\n",
            "logging_strategy=steps,\n",
            "lr_scheduler_type=cosine,\n",
            "max_grad_norm=1.0,\n",
            "max_steps=-1,\n",
            "metric_for_best_model=None,\n",
            "mp_parameters=,\n",
            "no_cuda=False,\n",
            "num_train_epochs=3.0,\n",
            "optim=adamw_torch,\n",
            "optim_args=None,\n",
            "output_dir=saves/Phi1.5-1.3B/lora/sam,\n",
            "overwrite_output_dir=False,\n",
            "past_index=-1,\n",
            "per_device_eval_batch_size=8,\n",
            "per_device_train_batch_size=1,\n",
            "predict_with_generate=False,\n",
            "prediction_loss_only=False,\n",
            "push_to_hub=False,\n",
            "push_to_hub_model_id=None,\n",
            "push_to_hub_organization=None,\n",
            "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
            "ray_scope=last,\n",
            "remove_unused_columns=True,\n",
            "report_to=['tensorboard'],\n",
            "resume_from_checkpoint=None,\n",
            "run_name=saves/Phi1.5-1.3B/lora/sam,\n",
            "save_on_each_node=False,\n",
            "save_safetensors=False,\n",
            "save_steps=100,\n",
            "save_strategy=steps,\n",
            "save_total_limit=None,\n",
            "seed=42,\n",
            "sharded_ddp=[],\n",
            "skip_memory_metrics=True,\n",
            "sortish_sampler=False,\n",
            "tf32=None,\n",
            "torch_compile=False,\n",
            "torch_compile_backend=None,\n",
            "torch_compile_mode=None,\n",
            "torchdynamo=None,\n",
            "tpu_metrics_debug=False,\n",
            "tpu_num_cores=None,\n",
            "use_cpu=False,\n",
            "use_ipex=False,\n",
            "use_legacy_prediction_loop=False,\n",
            "use_mps_device=False,\n",
            "warmup_ratio=0.0,\n",
            "warmup_steps=0,\n",
            "weight_decay=0.0,\n",
            ")\n",
            "11/21/2023 20:46:42 - INFO - llmtuner.data.loader - Loading dataset lima.json...\n",
            "Using custom data configuration default-712a4dc361df6b3c\n",
            "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
            "Overwrite dataset info from restored data version if exists.\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96)\n",
            "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:46:43,484 >> loading file vocab.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/vocab.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:46:43,484 >> loading file merges.txt from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/merges.txt\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:46:43,484 >> loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/tokenizer.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:46:43,484 >> loading file added_tokens.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/added_tokens.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:46:43,484 >> loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/special_tokens_map.json\n",
            "[INFO|tokenization_utils_base.py:2015] 2023-11-21 20:46:43,485 >> loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/tokenizer_config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-21 20:46:43,846 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/config.json\n",
            "[INFO|configuration_utils.py:715] 2023-11-21 20:46:44,057 >> loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/config.json\n",
            "[INFO|configuration_utils.py:775] 2023-11-21 20:46:44,058 >> Model config PhiConfig {\n",
            "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
            "  \"activation_function\": \"gelu_new\",\n",
            "  \"architectures\": [\n",
            "    \"PhiForCausalLM\"\n",
            "  ],\n",
            "  \"attn_pdrop\": 0.0,\n",
            "  \"auto_map\": {\n",
            "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_phi.PhiConfig\",\n",
            "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_phi.PhiForCausalLM\"\n",
            "  },\n",
            "  \"embd_pdrop\": 0.0,\n",
            "  \"flash_attn\": false,\n",
            "  \"flash_rotary\": false,\n",
            "  \"fused_dense\": false,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_epsilon\": 1e-05,\n",
            "  \"model_type\": \"phi\",\n",
            "  \"n_embd\": 2048,\n",
            "  \"n_head\": 32,\n",
            "  \"n_head_kv\": null,\n",
            "  \"n_inner\": null,\n",
            "  \"n_layer\": 24,\n",
            "  \"n_positions\": 2048,\n",
            "  \"resid_pdrop\": 0.0,\n",
            "  \"rotary_dim\": 32,\n",
            "  \"tie_word_embeddings\": false,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.34.1\",\n",
            "  \"vocab_size\": 51200\n",
            "}\n",
            "\n",
            "11/21/2023 20:46:44 - INFO - llmtuner.model.loader - Quantizing model to 4 bit.\n",
            "[INFO|modeling_utils.py:2993] 2023-11-21 20:46:44,177 >> loading weights file pytorch_model.bin from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/pytorch_model.bin\n",
            "[INFO|modeling_utils.py:1220] 2023-11-21 20:47:01,573 >> Instantiating PhiForCausalLM model under default dtype torch.float16.\n",
            "[INFO|configuration_utils.py:770] 2023-11-21 20:47:01,741 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|configuration_utils.py:770] 2023-11-21 20:47:01,742 >> Generate config GenerationConfig {}\n",
            "\n",
            "[INFO|modeling_utils.py:3103] 2023-11-21 20:47:01,804 >> Detected 4-bit loading: activating 4-bit loading for this model\n",
            "[INFO|modeling_utils.py:3775] 2023-11-21 20:47:11,709 >> All model checkpoint weights were used when initializing PhiForCausalLM.\n",
            "\n",
            "[INFO|modeling_utils.py:3783] 2023-11-21 20:47:11,709 >> All the weights of PhiForCausalLM were initialized from the model checkpoint at microsoft/phi-1_5.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use PhiForCausalLM for predictions without further training.\n",
            "[INFO|configuration_utils.py:730] 2023-11-21 20:47:11,820 >> loading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--microsoft--phi-1_5/snapshots/5fd430c7bcd28140560faee2014d1228338e19a0/generation_config.json\n",
            "[INFO|configuration_utils.py:770] 2023-11-21 20:47:11,820 >> Generate config GenerationConfig {}\n",
            "\n",
            "11/21/2023 20:47:11 - INFO - llmtuner.model.utils - Upcasting weights in layernorm in float32.\n",
            "11/21/2023 20:47:11 - INFO - llmtuner.model.utils - Using noisy embedding with alpha=1.00\n",
            "11/21/2023 20:47:11 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "11/21/2023 20:47:11 - INFO - llmtuner.model.loader - trainable params: 1572864 || all params: 1419843584 || trainable%: 0.1108\n",
            "11/21/2023 20:47:11 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "Running tokenizer on dataset:   0% 0/1029 [00:00<?, ? examples/s][WARNING|tokenization_utils_base.py:3823] 2023-11-21 20:47:12,992 >> Token indices sequence length is longer than the specified maximum sequence length for this model (3639 > 2048). Running this sequence through the model will result in indexing errors\n",
            "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-712a4dc361df6b3c/0.0.0/8bb11242116d547c741b2e8a1f18598ffdd40a1d4f2a2872c7a28b697434bc96/cache-cbc721e6af92a998.arrow\n",
            "Running tokenizer on dataset: 100% 1029/1029 [00:02<00:00, 459.13 examples/s]\n",
            "input_ids:\n",
            "[50256, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 628, 21017, 46486, 25, 198, 6090, 3632, 4778, 1445, 30, 2750, 3356, 314, 1612, 890, 464, 1808, 318, 5365, 3154, 290, 530, 815, 1011, 656, 1848, 326, 262, 3632, 407, 691, 10874, 286, 16890, 11, 475, 635, 1278, 498, 4778, 357, 11284, 425, 4778, 8, 290, 662, 12, 2781, 6210, 36347, 10717, 4778, 13, 11399, 11, 355, 4688, 5891, 12, 25346, 1023, 423, 8203, 11, 23456, 3800, 318, 845, 1593, 11, 355, 262, 5922, 49710, 3632, 318, 845, 1180, 422, 262, 4044, 3632, 13, 198, 4864, 11, 706, 264, 13309, 832, 2972, 16125, 11, 262, 3280, 284, 262, 1808, 318, 1682, 21196, 2829, 25, 3363, 11, 3632, 4778, 32492, 13, 198, 818, 50286, 1169, 4044, 3632, 1278, 498, 4778, 32492, 287, 262, 3632, 357, 42, 75, 11033, 2022, 83, 11, 3717, 737, 2671, 498, 4778, 389, 2950, 287, 257, 24862, 286, 5499, 11, 475, 257, 12411, 1672, 286, 45879, 1278, 498, 4778, 389, 262, 24869, 375, 437, 305, 948, 4879, 326, 32492, 3585, 890, 18868, 284, 1064, 511, 2496, 7877, 684, 4291, 543, 484, 14441, 2405, 284, 1296, 262, 1035, 8306, 616, 27176, 673, 776, 357, 33758, 1872, 290, 7920, 11, 6244, 737, 198, 8199, 333, 20996, 10717, 4778, 32492, 625, 890, 18868, 287, 2882, 284, 5095, 357, 40, 2781, 5708, 2123, 435, 1539, 5472, 8, 290, 484, 32492, 422, 2176, 10717, 12, 3846, 7064, 357, 68, 13, 70, 1539, 38587, 290, 850, 1151, 41001, 6516, 8]\n",
            "inputs:\n",
            "<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Can brain cells move? By movement I mean longThe question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
            "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
            "In  the adult brain glial cells migrate in the brain (Kl√§mbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
            "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone)\n",
            "label_ids:\n",
            "[50256, 21106, 318, 281, 12064, 326, 8477, 257, 4876, 13, 19430, 257, 2882, 326, 20431, 32543, 262, 2581, 13, 628, 21017, 46486, 25, 198, 6090, 3632, 4778, 1445, 30, 2750, 3356, 314, 1612, 890, 464, 1808, 318, 5365, 3154, 290, 530, 815, 1011, 656, 1848, 326, 262, 3632, 407, 691, 10874, 286, 16890, 11, 475, 635, 1278, 498, 4778, 357, 11284, 425, 4778, 8, 290, 662, 12, 2781, 6210, 36347, 10717, 4778, 13, 11399, 11, 355, 4688, 5891, 12, 25346, 1023, 423, 8203, 11, 23456, 3800, 318, 845, 1593, 11, 355, 262, 5922, 49710, 3632, 318, 845, 1180, 422, 262, 4044, 3632, 13, 198, 4864, 11, 706, 264, 13309, 832, 2972, 16125, 11, 262, 3280, 284, 262, 1808, 318, 1682, 21196, 2829, 25, 3363, 11, 3632, 4778, 32492, 13, 198, 818, 50286, 1169, 4044, 3632, 1278, 498, 4778, 32492, 287, 262, 3632, 357, 42, 75, 11033, 2022, 83, 11, 3717, 737, 2671, 498, 4778, 389, 2950, 287, 257, 24862, 286, 5499, 11, 475, 257, 12411, 1672, 286, 45879, 1278, 498, 4778, 389, 262, 24869, 375, 437, 305, 948, 4879, 326, 32492, 3585, 890, 18868, 284, 1064, 511, 2496, 7877, 684, 4291, 543, 484, 14441, 2405, 284, 1296, 262, 1035, 8306, 616, 27176, 673, 776, 357, 33758, 1872, 290, 7920, 11, 6244, 737, 198, 8199, 333, 20996, 10717, 4778, 32492, 625, 890, 18868, 287, 2882, 284, 5095, 357, 40, 2781, 5708, 2123, 435, 1539, 5472, 8, 290, 484, 32492, 422, 2176, 10717, 12, 3846, 7064, 357, 68, 13, 70, 1539, 38587, 290, 850, 1151, 41001, 6516, 8]\n",
            "labels:\n",
            "<|endoftext|>Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
            "\n",
            "### Instruction:\n",
            "Can brain cells move? By movement I mean longThe question is relatively broad and one should take into account that the brain not only consists of neurons, but also glial cells (supportive cells) and pre-mitotic neuronal stem cells. Furthermore, as critical fellow-scientists have indicated, developmental stage is very important, as the developing embryonic brain is very different from the adult brain.\n",
            "However, after sifting through various publications, the answer to the question is actually remarkably simple: Yes, brain cells migrate.\n",
            "In  the adult brain glial cells migrate in the brain (Kl√§mbt, 2009). Glial cells are involved in a myriad of functions, but a notable example of migrating glial cells are the oligodendrocytes that migrate relative long distances to find their target axons onto which they wrap themselves to form the insulating myelin sheath (Tsai and Miller, 2002).\n",
            "Neuronal stem cells migrate over long distances in response to injury (Imitola et al., 2004) and they migrate from specific stem-cell locations (e.g., hippocampus and subventricular zone)\n",
            "[INFO|training_args.py:1345] 2023-11-21 20:47:14,253 >> Found safetensors installation, but --save_safetensors=False. Safetensors should be a preferred weights saving format due to security and performance reasons. If your model cannot be saved by safetensors please feel free to open an issue at https://github.com/huggingface/safetensors!\n",
            "[INFO|training_args.py:1798] 2023-11-21 20:47:14,253 >> PyTorch: setting up devices\n",
            "[INFO|trainer.py:1760] 2023-11-21 20:47:14,569 >> ***** Running training *****\n",
            "[INFO|trainer.py:1761] 2023-11-21 20:47:14,569 >>   Num examples = 1,029\n",
            "[INFO|trainer.py:1762] 2023-11-21 20:47:14,569 >>   Num Epochs = 3\n",
            "[INFO|trainer.py:1763] 2023-11-21 20:47:14,569 >>   Instantaneous batch size per device = 1\n",
            "[INFO|trainer.py:1766] 2023-11-21 20:47:14,569 >>   Total train batch size (w. parallel, distributed & accumulation) = 1\n",
            "[INFO|trainer.py:1767] 2023-11-21 20:47:14,569 >>   Gradient Accumulation steps = 1\n",
            "[INFO|trainer.py:1768] 2023-11-21 20:47:14,569 >>   Total optimization steps = 3,087\n",
            "[INFO|trainer.py:1769] 2023-11-21 20:47:14,571 >>   Number of trainable parameters = 1,572,864\n",
            "[WARNING|logging.py:290] 2023-11-21 20:47:14,577 >> You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
            "11/21/2023 20:47:16 - INFO - llmtuner.extras.callbacks - {'loss': 3.1822, 'learning_rate': 5.0000e-05, 'epoch': 0.00}\n",
            "{'loss': 3.1822, 'learning_rate': 4.9999676350030046e-05, 'epoch': 0.0}\n",
            "11/21/2023 20:47:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.7799, 'learning_rate': 4.9999e-05, 'epoch': 0.01}\n",
            "{'loss': 2.7799, 'learning_rate': 4.9998705408500124e-05, 'epoch': 0.01}\n",
            "11/21/2023 20:47:19 - INFO - llmtuner.extras.callbacks - {'loss': 2.9244, 'learning_rate': 4.9997e-05, 'epoch': 0.01}\n",
            "{'loss': 2.9244, 'learning_rate': 4.9997087200549843e-05, 'epoch': 0.01}\n",
            "11/21/2023 20:47:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.6938, 'learning_rate': 4.9995e-05, 'epoch': 0.02}\n",
            "{'loss': 2.6938, 'learning_rate': 4.999482176807785e-05, 'epoch': 0.02}\n",
            "11/21/2023 20:47:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.8231, 'learning_rate': 4.9992e-05, 'epoch': 0.02}\n",
            "{'loss': 2.8231, 'learning_rate': 4.9991909169740716e-05, 'epoch': 0.02}\n",
            "11/21/2023 20:47:24 - INFO - llmtuner.extras.callbacks - {'loss': 3.1455, 'learning_rate': 4.9988e-05, 'epoch': 0.03}\n",
            "{'loss': 3.1455, 'learning_rate': 4.9988349480951424e-05, 'epoch': 0.03}\n",
            "11/21/2023 20:47:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.8410, 'learning_rate': 4.9984e-05, 'epoch': 0.03}\n",
            "{'loss': 2.841, 'learning_rate': 4.998414279387743e-05, 'epoch': 0.03}\n",
            "11/21/2023 20:47:27 - INFO - llmtuner.extras.callbacks - {'loss': 3.0264, 'learning_rate': 4.9979e-05, 'epoch': 0.04}\n",
            "{'loss': 3.0264, 'learning_rate': 4.9979289217438265e-05, 'epoch': 0.04}\n",
            "11/21/2023 20:47:28 - INFO - llmtuner.extras.callbacks - {'loss': 2.8752, 'learning_rate': 4.9974e-05, 'epoch': 0.04}\n",
            "{'loss': 2.8752, 'learning_rate': 4.997378887730272e-05, 'epoch': 0.04}\n",
            "11/21/2023 20:47:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.7719, 'learning_rate': 4.9968e-05, 'epoch': 0.05}\n",
            "{'loss': 2.7719, 'learning_rate': 4.996764191588559e-05, 'epoch': 0.05}\n",
            "11/21/2023 20:47:31 - INFO - llmtuner.extras.callbacks - {'loss': 2.5211, 'learning_rate': 4.9961e-05, 'epoch': 0.05}\n",
            "{'loss': 2.5211, 'learning_rate': 4.996084849234399e-05, 'epoch': 0.05}\n",
            "11/21/2023 20:47:33 - INFO - llmtuner.extras.callbacks - {'loss': 2.7300, 'learning_rate': 4.9953e-05, 'epoch': 0.06}\n",
            "{'loss': 2.73, 'learning_rate': 4.995340878257321e-05, 'epoch': 0.06}\n",
            "11/21/2023 20:47:34 - INFO - llmtuner.extras.callbacks - {'loss': 2.8663, 'learning_rate': 4.9945e-05, 'epoch': 0.06}\n",
            "{'loss': 2.8663, 'learning_rate': 4.994532297920221e-05, 'epoch': 0.06}\n",
            "11/21/2023 20:47:36 - INFO - llmtuner.extras.callbacks - {'loss': 3.0924, 'learning_rate': 4.9937e-05, 'epoch': 0.07}\n",
            "{'loss': 3.0924, 'learning_rate': 4.9936591291588586e-05, 'epoch': 0.07}\n",
            "11/21/2023 20:47:38 - INFO - llmtuner.extras.callbacks - {'loss': 2.5539, 'learning_rate': 4.9927e-05, 'epoch': 0.07}\n",
            "{'loss': 2.5539, 'learning_rate': 4.992721394581318e-05, 'epoch': 0.07}\n",
            "11/21/2023 20:47:40 - INFO - llmtuner.extras.callbacks - {'loss': 2.1041, 'learning_rate': 4.9917e-05, 'epoch': 0.08}\n",
            "{'loss': 2.1041, 'learning_rate': 4.99171911846742e-05, 'epoch': 0.08}\n",
            "11/21/2023 20:47:41 - INFO - llmtuner.extras.callbacks - {'loss': 2.7866, 'learning_rate': 4.9907e-05, 'epoch': 0.08}\n",
            "{'loss': 2.7866, 'learning_rate': 4.990652326768096e-05, 'epoch': 0.08}\n",
            "11/21/2023 20:47:43 - INFO - llmtuner.extras.callbacks - {'loss': 2.4475, 'learning_rate': 4.9895e-05, 'epoch': 0.09}\n",
            "{'loss': 2.4475, 'learning_rate': 4.989521047104713e-05, 'epoch': 0.09}\n",
            "11/21/2023 20:47:44 - INFO - llmtuner.extras.callbacks - {'loss': 2.6629, 'learning_rate': 4.9883e-05, 'epoch': 0.09}\n",
            "{'loss': 2.6629, 'learning_rate': 4.988325308768362e-05, 'epoch': 0.09}\n",
            "11/21/2023 20:47:46 - INFO - llmtuner.extras.callbacks - {'loss': 2.5234, 'learning_rate': 4.9871e-05, 'epoch': 0.10}\n",
            "{'loss': 2.5234, 'learning_rate': 4.9870651427190966e-05, 'epoch': 0.1}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:47:46,320 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-100\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:47:46,368 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:47:46,368 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-100/special_tokens_map.json\n",
            "11/21/2023 20:47:48 - INFO - llmtuner.extras.callbacks - {'loss': 2.6593, 'learning_rate': 4.9857e-05, 'epoch': 0.10}\n",
            "{'loss': 2.6593, 'learning_rate': 4.985740581585134e-05, 'epoch': 0.1}\n",
            "11/21/2023 20:47:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.8399, 'learning_rate': 4.9844e-05, 'epoch': 0.11}\n",
            "{'loss': 2.8399, 'learning_rate': 4.984351659662008e-05, 'epoch': 0.11}\n",
            "11/21/2023 20:47:51 - INFO - llmtuner.extras.callbacks - {'loss': 2.2143, 'learning_rate': 4.9829e-05, 'epoch': 0.11}\n",
            "{'loss': 2.2143, 'learning_rate': 4.9828984129116806e-05, 'epoch': 0.11}\n",
            "11/21/2023 20:47:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.5913, 'learning_rate': 4.9814e-05, 'epoch': 0.12}\n",
            "{'loss': 2.5913, 'learning_rate': 4.981380878961614e-05, 'epoch': 0.12}\n",
            "11/21/2023 20:47:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.0668, 'learning_rate': 4.9798e-05, 'epoch': 0.12}\n",
            "{'loss': 2.0668, 'learning_rate': 4.9797990971037926e-05, 'epoch': 0.12}\n",
            "11/21/2023 20:47:56 - INFO - llmtuner.extras.callbacks - {'loss': 2.9903, 'learning_rate': 4.9782e-05, 'epoch': 0.13}\n",
            "{'loss': 2.9903, 'learning_rate': 4.97815310829371e-05, 'epoch': 0.13}\n",
            "11/21/2023 20:47:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.4306, 'learning_rate': 4.9764e-05, 'epoch': 0.13}\n",
            "{'loss': 2.4306, 'learning_rate': 4.976442955149303e-05, 'epoch': 0.13}\n",
            "11/21/2023 20:47:59 - INFO - llmtuner.extras.callbacks - {'loss': 2.4013, 'learning_rate': 4.9747e-05, 'epoch': 0.14}\n",
            "{'loss': 2.4013, 'learning_rate': 4.9746686819498546e-05, 'epoch': 0.14}\n",
            "11/21/2023 20:48:00 - INFO - llmtuner.extras.callbacks - {'loss': 2.4738, 'learning_rate': 4.9728e-05, 'epoch': 0.14}\n",
            "{'loss': 2.4738, 'learning_rate': 4.9728303346348395e-05, 'epoch': 0.14}\n",
            "11/21/2023 20:48:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.0462, 'learning_rate': 4.9709e-05, 'epoch': 0.15}\n",
            "{'loss': 2.0462, 'learning_rate': 4.970927960802745e-05, 'epoch': 0.15}\n",
            "11/21/2023 20:48:03 - INFO - llmtuner.extras.callbacks - {'loss': 2.3287, 'learning_rate': 4.9690e-05, 'epoch': 0.15}\n",
            "{'loss': 2.3287, 'learning_rate': 4.968961609709827e-05, 'epoch': 0.15}\n",
            "11/21/2023 20:48:05 - INFO - llmtuner.extras.callbacks - {'loss': 2.4005, 'learning_rate': 4.9669e-05, 'epoch': 0.16}\n",
            "{'loss': 2.4005, 'learning_rate': 4.966931332268845e-05, 'epoch': 0.16}\n",
            "11/21/2023 20:48:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.5544, 'learning_rate': 4.9648e-05, 'epoch': 0.16}\n",
            "{'loss': 2.5544, 'learning_rate': 4.9648371810477375e-05, 'epoch': 0.16}\n",
            "11/21/2023 20:48:08 - INFO - llmtuner.extras.callbacks - {'loss': 2.3910, 'learning_rate': 4.9627e-05, 'epoch': 0.17}\n",
            "{'loss': 2.391, 'learning_rate': 4.9626792102682615e-05, 'epoch': 0.17}\n",
            "11/21/2023 20:48:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.6672, 'learning_rate': 4.9605e-05, 'epoch': 0.17}\n",
            "{'loss': 2.6672, 'learning_rate': 4.960457475804594e-05, 'epoch': 0.17}\n",
            "11/21/2023 20:48:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.1988, 'learning_rate': 4.9582e-05, 'epoch': 0.17}\n",
            "{'loss': 2.1988, 'learning_rate': 4.958172035181875e-05, 'epoch': 0.17}\n",
            "11/21/2023 20:48:14 - INFO - llmtuner.extras.callbacks - {'loss': 2.2514, 'learning_rate': 4.9558e-05, 'epoch': 0.18}\n",
            "{'loss': 2.2514, 'learning_rate': 4.9558229475747305e-05, 'epoch': 0.18}\n",
            "11/21/2023 20:48:15 - INFO - llmtuner.extras.callbacks - {'loss': 2.5901, 'learning_rate': 4.9534e-05, 'epoch': 0.18}\n",
            "{'loss': 2.5901, 'learning_rate': 4.9534102738057296e-05, 'epoch': 0.18}\n",
            "11/21/2023 20:48:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.3491, 'learning_rate': 4.9509e-05, 'epoch': 0.19}\n",
            "{'loss': 2.3491, 'learning_rate': 4.950934076343816e-05, 'epoch': 0.19}\n",
            "11/21/2023 20:48:18 - INFO - llmtuner.extras.callbacks - {'loss': 2.3425, 'learning_rate': 4.9484e-05, 'epoch': 0.19}\n",
            "{'loss': 2.3425, 'learning_rate': 4.9483944193026885e-05, 'epoch': 0.19}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:48:18,805 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-200\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:48:18,845 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:48:18,845 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-200/special_tokens_map.json\n",
            "11/21/2023 20:48:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.7003, 'learning_rate': 4.9458e-05, 'epoch': 0.20}\n",
            "{'loss': 2.7003, 'learning_rate': 4.945791368439141e-05, 'epoch': 0.2}\n",
            "11/21/2023 20:48:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.0280, 'learning_rate': 4.9431e-05, 'epoch': 0.20}\n",
            "{'loss': 2.028, 'learning_rate': 4.94312499115136e-05, 'epoch': 0.2}\n",
            "11/21/2023 20:48:23 - INFO - llmtuner.extras.callbacks - {'loss': 2.2368, 'learning_rate': 4.9404e-05, 'epoch': 0.21}\n",
            "{'loss': 2.2368, 'learning_rate': 4.940395356477181e-05, 'epoch': 0.21}\n",
            "11/21/2023 20:48:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.2598, 'learning_rate': 4.9376e-05, 'epoch': 0.21}\n",
            "{'loss': 2.2598, 'learning_rate': 4.937602535092297e-05, 'epoch': 0.21}\n",
            "11/21/2023 20:48:26 - INFO - llmtuner.extras.callbacks - {'loss': 2.0020, 'learning_rate': 4.9347e-05, 'epoch': 0.22}\n",
            "{'loss': 2.002, 'learning_rate': 4.934746599308434e-05, 'epoch': 0.22}\n",
            "11/21/2023 20:48:28 - INFO - llmtuner.extras.callbacks - {'loss': 2.8813, 'learning_rate': 4.9318e-05, 'epoch': 0.22}\n",
            "{'loss': 2.8813, 'learning_rate': 4.931827623071472e-05, 'epoch': 0.22}\n",
            "11/21/2023 20:48:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.3316, 'learning_rate': 4.9288e-05, 'epoch': 0.23}\n",
            "{'loss': 2.3316, 'learning_rate': 4.9288456819595396e-05, 'epoch': 0.23}\n",
            "11/21/2023 20:48:31 - INFO - llmtuner.extras.callbacks - {'loss': 2.4666, 'learning_rate': 4.9258e-05, 'epoch': 0.23}\n",
            "{'loss': 2.4666, 'learning_rate': 4.925800853181047e-05, 'epoch': 0.23}\n",
            "11/21/2023 20:48:33 - INFO - llmtuner.extras.callbacks - {'loss': 2.4319, 'learning_rate': 4.9227e-05, 'epoch': 0.24}\n",
            "{'loss': 2.4319, 'learning_rate': 4.922693215572695e-05, 'epoch': 0.24}\n",
            "11/21/2023 20:48:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.1890, 'learning_rate': 4.9195e-05, 'epoch': 0.24}\n",
            "{'loss': 2.189, 'learning_rate': 4.919522849597428e-05, 'epoch': 0.24}\n",
            "11/21/2023 20:48:36 - INFO - llmtuner.extras.callbacks - {'loss': 2.1418, 'learning_rate': 4.9163e-05, 'epoch': 0.25}\n",
            "{'loss': 2.1418, 'learning_rate': 4.9162898373423546e-05, 'epoch': 0.25}\n",
            "11/21/2023 20:48:38 - INFO - llmtuner.extras.callbacks - {'loss': 1.8529, 'learning_rate': 4.9130e-05, 'epoch': 0.25}\n",
            "{'loss': 1.8529, 'learning_rate': 4.9129942625166206e-05, 'epoch': 0.25}\n",
            "11/21/2023 20:48:40 - INFO - llmtuner.extras.callbacks - {'loss': 2.0382, 'learning_rate': 4.9096e-05, 'epoch': 0.26}\n",
            "{'loss': 2.0382, 'learning_rate': 4.90963621044924e-05, 'epoch': 0.26}\n",
            "11/21/2023 20:48:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.5267, 'learning_rate': 4.9062e-05, 'epoch': 0.26}\n",
            "{'loss': 2.5267, 'learning_rate': 4.906215768086891e-05, 'epoch': 0.26}\n",
            "11/21/2023 20:48:43 - INFO - llmtuner.extras.callbacks - {'loss': 2.2330, 'learning_rate': 4.9027e-05, 'epoch': 0.27}\n",
            "{'loss': 2.233, 'learning_rate': 4.902733023991658e-05, 'epoch': 0.27}\n",
            "11/21/2023 20:48:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.3740, 'learning_rate': 4.8992e-05, 'epoch': 0.27}\n",
            "{'loss': 2.374, 'learning_rate': 4.899188068338743e-05, 'epoch': 0.27}\n",
            "11/21/2023 20:48:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.3861, 'learning_rate': 4.8956e-05, 'epoch': 0.28}\n",
            "{'loss': 2.3861, 'learning_rate': 4.895580992914128e-05, 'epoch': 0.28}\n",
            "11/21/2023 20:48:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.5648, 'learning_rate': 4.8919e-05, 'epoch': 0.28}\n",
            "{'loss': 2.5648, 'learning_rate': 4.891911891112203e-05, 'epoch': 0.28}\n",
            "11/21/2023 20:48:50 - INFO - llmtuner.extras.callbacks - {'loss': 2.3976, 'learning_rate': 4.8882e-05, 'epoch': 0.29}\n",
            "{'loss': 2.3976, 'learning_rate': 4.8881808579333414e-05, 'epoch': 0.29}\n",
            "11/21/2023 20:48:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.0647, 'learning_rate': 4.8844e-05, 'epoch': 0.29}\n",
            "{'loss': 2.0647, 'learning_rate': 4.884387989981447e-05, 'epoch': 0.29}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:48:52,345 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-300\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:48:52,387 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:48:52,387 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-300/special_tokens_map.json\n",
            "11/21/2023 20:48:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.3506, 'learning_rate': 4.8805e-05, 'epoch': 0.30}\n",
            "{'loss': 2.3506, 'learning_rate': 4.8805333854614463e-05, 'epoch': 0.3}\n",
            "11/21/2023 20:48:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.5230, 'learning_rate': 4.8766e-05, 'epoch': 0.30}\n",
            "{'loss': 2.523, 'learning_rate': 4.8766171441767514e-05, 'epoch': 0.3}\n",
            "11/21/2023 20:48:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.7292, 'learning_rate': 4.8726e-05, 'epoch': 0.31}\n",
            "{'loss': 2.7292, 'learning_rate': 4.8726393675266716e-05, 'epoch': 0.31}\n",
            "11/21/2023 20:48:58 - INFO - llmtuner.extras.callbacks - {'loss': 2.2454, 'learning_rate': 4.8686e-05, 'epoch': 0.31}\n",
            "{'loss': 2.2454, 'learning_rate': 4.868600158503791e-05, 'epoch': 0.31}\n",
            "11/21/2023 20:49:00 - INFO - llmtuner.extras.callbacks - {'loss': 2.2059, 'learning_rate': 4.8645e-05, 'epoch': 0.32}\n",
            "{'loss': 2.2059, 'learning_rate': 4.864499621691298e-05, 'epoch': 0.32}\n",
            "11/21/2023 20:49:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.0915, 'learning_rate': 4.8603e-05, 'epoch': 0.32}\n",
            "{'loss': 2.0915, 'learning_rate': 4.8603378632602846e-05, 'epoch': 0.32}\n",
            "11/21/2023 20:49:04 - INFO - llmtuner.extras.callbacks - {'loss': 2.6273, 'learning_rate': 4.8561e-05, 'epoch': 0.33}\n",
            "{'loss': 2.6273, 'learning_rate': 4.856114990966988e-05, 'epoch': 0.33}\n",
            "11/21/2023 20:49:05 - INFO - llmtuner.extras.callbacks - {'loss': 2.4401, 'learning_rate': 4.8518e-05, 'epoch': 0.33}\n",
            "{'loss': 2.4401, 'learning_rate': 4.851831114150008e-05, 'epoch': 0.33}\n",
            "11/21/2023 20:49:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.6195, 'learning_rate': 4.8475e-05, 'epoch': 0.34}\n",
            "{'loss': 2.6195, 'learning_rate': 4.847486343727474e-05, 'epoch': 0.34}\n",
            "11/21/2023 20:49:09 - INFO - llmtuner.extras.callbacks - {'loss': 2.4261, 'learning_rate': 4.8431e-05, 'epoch': 0.34}\n",
            "{'loss': 2.4261, 'learning_rate': 4.84308079219417e-05, 'epoch': 0.34}\n",
            "11/21/2023 20:49:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.3768, 'learning_rate': 4.8386e-05, 'epoch': 0.34}\n",
            "{'loss': 2.3768, 'learning_rate': 4.838614573618625e-05, 'epoch': 0.34}\n",
            "11/21/2023 20:49:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.6350, 'learning_rate': 4.8341e-05, 'epoch': 0.35}\n",
            "{'loss': 2.635, 'learning_rate': 4.834087803640162e-05, 'epoch': 0.35}\n",
            "11/21/2023 20:49:13 - INFO - llmtuner.extras.callbacks - {'loss': 2.4060, 'learning_rate': 4.8295e-05, 'epoch': 0.35}\n",
            "{'loss': 2.406, 'learning_rate': 4.829500599465896e-05, 'epoch': 0.35}\n",
            "11/21/2023 20:49:15 - INFO - llmtuner.extras.callbacks - {'loss': 2.5057, 'learning_rate': 4.8249e-05, 'epoch': 0.36}\n",
            "{'loss': 2.5057, 'learning_rate': 4.824853079867709e-05, 'epoch': 0.36}\n",
            "11/21/2023 20:49:16 - INFO - llmtuner.extras.callbacks - {'loss': 2.5930, 'learning_rate': 4.8201e-05, 'epoch': 0.36}\n",
            "{'loss': 2.593, 'learning_rate': 4.820145365179165e-05, 'epoch': 0.36}\n",
            "11/21/2023 20:49:18 - INFO - llmtuner.extras.callbacks - {'loss': 2.3510, 'learning_rate': 4.8154e-05, 'epoch': 0.37}\n",
            "{'loss': 2.351, 'learning_rate': 4.815377577292403e-05, 'epoch': 0.37}\n",
            "11/21/2023 20:49:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.3868, 'learning_rate': 4.8105e-05, 'epoch': 0.37}\n",
            "{'loss': 2.3868, 'learning_rate': 4.810549839654973e-05, 'epoch': 0.37}\n",
            "11/21/2023 20:49:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.0931, 'learning_rate': 4.8057e-05, 'epoch': 0.38}\n",
            "{'loss': 2.0931, 'learning_rate': 4.80566227726665e-05, 'epoch': 0.38}\n",
            "11/21/2023 20:49:24 - INFO - llmtuner.extras.callbacks - {'loss': 2.3221, 'learning_rate': 4.8007e-05, 'epoch': 0.38}\n",
            "{'loss': 2.3221, 'learning_rate': 4.800715016676185e-05, 'epoch': 0.38}\n",
            "11/21/2023 20:49:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.1979, 'learning_rate': 4.7957e-05, 'epoch': 0.39}\n",
            "{'loss': 2.1979, 'learning_rate': 4.795708185978037e-05, 'epoch': 0.39}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:49:25,557 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-400\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:49:25,599 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:49:25,599 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-400/special_tokens_map.json\n",
            "11/21/2023 20:49:27 - INFO - llmtuner.extras.callbacks - {'loss': 2.1338, 'learning_rate': 4.7906e-05, 'epoch': 0.39}\n",
            "{'loss': 2.1338, 'learning_rate': 4.790641914809057e-05, 'epoch': 0.39}\n",
            "11/21/2023 20:49:29 - INFO - llmtuner.extras.callbacks - {'loss': 2.4214, 'learning_rate': 4.7855e-05, 'epoch': 0.40}\n",
            "{'loss': 2.4214, 'learning_rate': 4.7855163343451234e-05, 'epoch': 0.4}\n",
            "11/21/2023 20:49:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.6266, 'learning_rate': 4.7803e-05, 'epoch': 0.40}\n",
            "{'loss': 2.6266, 'learning_rate': 4.780331577297754e-05, 'epoch': 0.4}\n",
            "11/21/2023 20:49:32 - INFO - llmtuner.extras.callbacks - {'loss': 2.1456, 'learning_rate': 4.7751e-05, 'epoch': 0.41}\n",
            "{'loss': 2.1456, 'learning_rate': 4.7750877779106666e-05, 'epoch': 0.41}\n",
            "11/21/2023 20:49:33 - INFO - llmtuner.extras.callbacks - {'loss': 2.7532, 'learning_rate': 4.7698e-05, 'epoch': 0.41}\n",
            "{'loss': 2.7532, 'learning_rate': 4.769785071956302e-05, 'epoch': 0.41}\n",
            "11/21/2023 20:49:35 - INFO - llmtuner.extras.callbacks - {'loss': 1.8490, 'learning_rate': 4.7644e-05, 'epoch': 0.42}\n",
            "{'loss': 1.849, 'learning_rate': 4.7644235967323094e-05, 'epoch': 0.42}\n",
            "11/21/2023 20:49:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.1469, 'learning_rate': 4.7590e-05, 'epoch': 0.42}\n",
            "{'loss': 2.1469, 'learning_rate': 4.759003491057992e-05, 'epoch': 0.42}\n",
            "11/21/2023 20:49:38 - INFO - llmtuner.extras.callbacks - {'loss': 2.6141, 'learning_rate': 4.7535e-05, 'epoch': 0.43}\n",
            "{'loss': 2.6141, 'learning_rate': 4.753524895270715e-05, 'epoch': 0.43}\n",
            "11/21/2023 20:49:40 - INFO - llmtuner.extras.callbacks - {'loss': 2.0476, 'learning_rate': 4.7480e-05, 'epoch': 0.43}\n",
            "{'loss': 2.0476, 'learning_rate': 4.7479879512222645e-05, 'epoch': 0.43}\n",
            "11/21/2023 20:49:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.4669, 'learning_rate': 4.7424e-05, 'epoch': 0.44}\n",
            "{'loss': 2.4669, 'learning_rate': 4.742392802275185e-05, 'epoch': 0.44}\n",
            "11/21/2023 20:49:43 - INFO - llmtuner.extras.callbacks - {'loss': 2.5857, 'learning_rate': 4.7367e-05, 'epoch': 0.44}\n",
            "{'loss': 2.5857, 'learning_rate': 4.736739593299058e-05, 'epoch': 0.44}\n",
            "11/21/2023 20:49:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.6970, 'learning_rate': 4.7310e-05, 'epoch': 0.45}\n",
            "{'loss': 2.697, 'learning_rate': 4.7310284706667577e-05, 'epoch': 0.45}\n",
            "11/21/2023 20:49:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.1550, 'learning_rate': 4.7253e-05, 'epoch': 0.45}\n",
            "{'loss': 2.155, 'learning_rate': 4.725259582250656e-05, 'epoch': 0.45}\n",
            "11/21/2023 20:49:48 - INFO - llmtuner.extras.callbacks - {'loss': 2.6607, 'learning_rate': 4.7194e-05, 'epoch': 0.46}\n",
            "{'loss': 2.6607, 'learning_rate': 4.7194330774188004e-05, 'epoch': 0.46}\n",
            "11/21/2023 20:49:50 - INFO - llmtuner.extras.callbacks - {'loss': 2.8731, 'learning_rate': 4.7135e-05, 'epoch': 0.46}\n",
            "{'loss': 2.8731, 'learning_rate': 4.713549107031039e-05, 'epoch': 0.46}\n",
            "11/21/2023 20:49:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.4566, 'learning_rate': 4.7076e-05, 'epoch': 0.47}\n",
            "{'loss': 2.4566, 'learning_rate': 4.707607823435118e-05, 'epoch': 0.47}\n",
            "11/21/2023 20:49:53 - INFO - llmtuner.extras.callbacks - {'loss': 2.6231, 'learning_rate': 4.7016e-05, 'epoch': 0.47}\n",
            "{'loss': 2.6231, 'learning_rate': 4.701609380462738e-05, 'epoch': 0.47}\n",
            "11/21/2023 20:49:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.4990, 'learning_rate': 4.6956e-05, 'epoch': 0.48}\n",
            "{'loss': 2.499, 'learning_rate': 4.6955539334255716e-05, 'epoch': 0.48}\n",
            "11/21/2023 20:49:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.6732, 'learning_rate': 4.6894e-05, 'epoch': 0.48}\n",
            "{'loss': 2.6732, 'learning_rate': 4.6894416391112374e-05, 'epoch': 0.48}\n",
            "11/21/2023 20:49:59 - INFO - llmtuner.extras.callbacks - {'loss': 1.7529, 'learning_rate': 4.6833e-05, 'epoch': 0.49}\n",
            "{'loss': 1.7529, 'learning_rate': 4.683272655779246e-05, 'epoch': 0.49}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:49:59,277 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-500\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:49:59,321 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:49:59,321 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-500/special_tokens_map.json\n",
            "11/21/2023 20:50:00 - INFO - llmtuner.extras.callbacks - {'loss': 2.1945, 'learning_rate': 4.6770e-05, 'epoch': 0.49}\n",
            "{'loss': 2.1945, 'learning_rate': 4.677047143156899e-05, 'epoch': 0.49}\n",
            "11/21/2023 20:50:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.4266, 'learning_rate': 4.6708e-05, 'epoch': 0.50}\n",
            "{'loss': 2.4266, 'learning_rate': 4.6707652624351525e-05, 'epoch': 0.5}\n",
            "11/21/2023 20:50:04 - INFO - llmtuner.extras.callbacks - {'loss': 1.6222, 'learning_rate': 4.6644e-05, 'epoch': 0.50}\n",
            "{'loss': 1.6222, 'learning_rate': 4.6644271762644495e-05, 'epoch': 0.5}\n",
            "11/21/2023 20:50:05 - INFO - llmtuner.extras.callbacks - {'loss': 2.5486, 'learning_rate': 4.6580e-05, 'epoch': 0.51}\n",
            "{'loss': 2.5486, 'learning_rate': 4.658033048750501e-05, 'epoch': 0.51}\n",
            "11/21/2023 20:50:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.7695, 'learning_rate': 4.6516e-05, 'epoch': 0.51}\n",
            "{'loss': 2.7695, 'learning_rate': 4.651583045450041e-05, 'epoch': 0.51}\n",
            "11/21/2023 20:50:08 - INFO - llmtuner.extras.callbacks - {'loss': 2.2565, 'learning_rate': 4.6451e-05, 'epoch': 0.52}\n",
            "{'loss': 2.2565, 'learning_rate': 4.645077333366539e-05, 'epoch': 0.52}\n",
            "11/21/2023 20:50:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.0366, 'learning_rate': 4.6385e-05, 'epoch': 0.52}\n",
            "{'loss': 2.0366, 'learning_rate': 4.638516080945878e-05, 'epoch': 0.52}\n",
            "11/21/2023 20:50:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.6791, 'learning_rate': 4.6319e-05, 'epoch': 0.52}\n",
            "{'loss': 2.6791, 'learning_rate': 4.6318994580719885e-05, 'epoch': 0.52}\n",
            "11/21/2023 20:50:13 - INFO - llmtuner.extras.callbacks - {'loss': 2.1683, 'learning_rate': 4.6252e-05, 'epoch': 0.53}\n",
            "{'loss': 2.1683, 'learning_rate': 4.625227636062455e-05, 'epoch': 0.53}\n",
            "11/21/2023 20:50:15 - INFO - llmtuner.extras.callbacks - {'loss': 2.0377, 'learning_rate': 4.6185e-05, 'epoch': 0.53}\n",
            "{'loss': 2.0377, 'learning_rate': 4.6185007876640765e-05, 'epoch': 0.53}\n",
            "11/21/2023 20:50:16 - INFO - llmtuner.extras.callbacks - {'loss': 2.0736, 'learning_rate': 4.6117e-05, 'epoch': 0.54}\n",
            "{'loss': 2.0736, 'learning_rate': 4.611719087048395e-05, 'epoch': 0.54}\n",
            "11/21/2023 20:50:18 - INFO - llmtuner.extras.callbacks - {'loss': 2.1549, 'learning_rate': 4.6049e-05, 'epoch': 0.54}\n",
            "{'loss': 2.1549, 'learning_rate': 4.604882709807187e-05, 'epoch': 0.54}\n",
            "11/21/2023 20:50:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.5816, 'learning_rate': 4.5980e-05, 'epoch': 0.55}\n",
            "{'loss': 2.5816, 'learning_rate': 4.5979918329479166e-05, 'epoch': 0.55}\n",
            "11/21/2023 20:50:21 - INFO - llmtuner.extras.callbacks - {'loss': 2.7209, 'learning_rate': 4.5910e-05, 'epoch': 0.55}\n",
            "{'loss': 2.7209, 'learning_rate': 4.59104663488915e-05, 'epoch': 0.55}\n",
            "11/21/2023 20:50:23 - INFO - llmtuner.extras.callbacks - {'loss': 2.0577, 'learning_rate': 4.5840e-05, 'epoch': 0.56}\n",
            "{'loss': 2.0577, 'learning_rate': 4.584047295455938e-05, 'epoch': 0.56}\n",
            "11/21/2023 20:50:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.5655, 'learning_rate': 4.5770e-05, 'epoch': 0.56}\n",
            "{'loss': 2.5655, 'learning_rate': 4.5769939958751616e-05, 'epoch': 0.56}\n",
            "11/21/2023 20:50:27 - INFO - llmtuner.extras.callbacks - {'loss': 2.3130, 'learning_rate': 4.5699e-05, 'epoch': 0.57}\n",
            "{'loss': 2.313, 'learning_rate': 4.569886918770836e-05, 'epoch': 0.57}\n",
            "11/21/2023 20:50:28 - INFO - llmtuner.extras.callbacks - {'loss': 2.6316, 'learning_rate': 4.5627e-05, 'epoch': 0.57}\n",
            "{'loss': 2.6316, 'learning_rate': 4.562726248159385e-05, 'epoch': 0.57}\n",
            "11/21/2023 20:50:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.2471, 'learning_rate': 4.5555e-05, 'epoch': 0.58}\n",
            "{'loss': 2.2471, 'learning_rate': 4.5555121694448735e-05, 'epoch': 0.58}\n",
            "11/21/2023 20:50:32 - INFO - llmtuner.extras.callbacks - {'loss': 2.4437, 'learning_rate': 4.5482e-05, 'epoch': 0.58}\n",
            "{'loss': 2.4437, 'learning_rate': 4.548244869414211e-05, 'epoch': 0.58}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:50:32,563 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-600\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:50:32,637 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-600/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:50:32,637 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-600/special_tokens_map.json\n",
            "11/21/2023 20:50:34 - INFO - llmtuner.extras.callbacks - {'loss': 1.8839, 'learning_rate': 4.5409e-05, 'epoch': 0.59}\n",
            "{'loss': 1.8839, 'learning_rate': 4.5409245362323136e-05, 'epoch': 0.59}\n",
            "11/21/2023 20:50:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.7009, 'learning_rate': 4.5336e-05, 'epoch': 0.59}\n",
            "{'loss': 2.7009, 'learning_rate': 4.533551359437229e-05, 'epoch': 0.59}\n",
            "11/21/2023 20:50:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.2581, 'learning_rate': 4.5261e-05, 'epoch': 0.60}\n",
            "{'loss': 2.2581, 'learning_rate': 4.5261255299352336e-05, 'epoch': 0.6}\n",
            "11/21/2023 20:50:39 - INFO - llmtuner.extras.callbacks - {'loss': 2.2846, 'learning_rate': 4.5186e-05, 'epoch': 0.60}\n",
            "{'loss': 2.2846, 'learning_rate': 4.518647239995887e-05, 'epoch': 0.6}\n",
            "11/21/2023 20:50:40 - INFO - llmtuner.extras.callbacks - {'loss': 2.3429, 'learning_rate': 4.5111e-05, 'epoch': 0.61}\n",
            "{'loss': 2.3429, 'learning_rate': 4.5111166832470536e-05, 'epoch': 0.61}\n",
            "11/21/2023 20:50:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.5248, 'learning_rate': 4.5035e-05, 'epoch': 0.61}\n",
            "{'loss': 2.5248, 'learning_rate': 4.503534054669892e-05, 'epoch': 0.61}\n",
            "11/21/2023 20:50:43 - INFO - llmtuner.extras.callbacks - {'loss': 1.9899, 'learning_rate': 4.4959e-05, 'epoch': 0.62}\n",
            "{'loss': 1.9899, 'learning_rate': 4.495899550593802e-05, 'epoch': 0.62}\n",
            "11/21/2023 20:50:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.0810, 'learning_rate': 4.4882e-05, 'epoch': 0.62}\n",
            "{'loss': 2.081, 'learning_rate': 4.488213368691345e-05, 'epoch': 0.62}\n",
            "11/21/2023 20:50:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.1600, 'learning_rate': 4.4805e-05, 'epoch': 0.63}\n",
            "{'loss': 2.16, 'learning_rate': 4.480475707973125e-05, 'epoch': 0.63}\n",
            "11/21/2023 20:50:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.6348, 'learning_rate': 4.4727e-05, 'epoch': 0.63}\n",
            "{'loss': 2.6348, 'learning_rate': 4.472686768782634e-05, 'epoch': 0.63}\n",
            "11/21/2023 20:50:50 - INFO - llmtuner.extras.callbacks - {'loss': 2.1375, 'learning_rate': 4.4648e-05, 'epoch': 0.64}\n",
            "{'loss': 2.1375, 'learning_rate': 4.4648467527910676e-05, 'epoch': 0.64}\n",
            "11/21/2023 20:50:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.1395, 'learning_rate': 4.4570e-05, 'epoch': 0.64}\n",
            "{'loss': 2.1395, 'learning_rate': 4.4569558629920994e-05, 'epoch': 0.64}\n",
            "11/21/2023 20:50:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.7265, 'learning_rate': 4.4490e-05, 'epoch': 0.65}\n",
            "{'loss': 2.7265, 'learning_rate': 4.44901430369663e-05, 'epoch': 0.65}\n",
            "11/21/2023 20:50:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.8129, 'learning_rate': 4.4410e-05, 'epoch': 0.65}\n",
            "{'loss': 2.8129, 'learning_rate': 4.4410222805274945e-05, 'epoch': 0.65}\n",
            "11/21/2023 20:50:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.4390, 'learning_rate': 4.4330e-05, 'epoch': 0.66}\n",
            "{'loss': 2.439, 'learning_rate': 4.432980000414138e-05, 'epoch': 0.66}\n",
            "11/21/2023 20:50:58 - INFO - llmtuner.extras.callbacks - {'loss': 2.3221, 'learning_rate': 4.4249e-05, 'epoch': 0.66}\n",
            "{'loss': 2.3221, 'learning_rate': 4.424887671587254e-05, 'epoch': 0.66}\n",
            "11/21/2023 20:51:00 - INFO - llmtuner.extras.callbacks - {'loss': 2.0690, 'learning_rate': 4.4167e-05, 'epoch': 0.67}\n",
            "{'loss': 2.069, 'learning_rate': 4.4167455035734054e-05, 'epoch': 0.67}\n",
            "11/21/2023 20:51:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.1973, 'learning_rate': 4.4086e-05, 'epoch': 0.67}\n",
            "{'loss': 2.1973, 'learning_rate': 4.4085537071895856e-05, 'epoch': 0.67}\n",
            "11/21/2023 20:51:03 - INFO - llmtuner.extras.callbacks - {'loss': 2.4849, 'learning_rate': 4.4003e-05, 'epoch': 0.68}\n",
            "{'loss': 2.4849, 'learning_rate': 4.400312494537766e-05, 'epoch': 0.68}\n",
            "11/21/2023 20:51:05 - INFO - llmtuner.extras.callbacks - {'loss': 2.0616, 'learning_rate': 4.3920e-05, 'epoch': 0.68}\n",
            "{'loss': 2.0616, 'learning_rate': 4.392022078999405e-05, 'epoch': 0.68}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:51:05,578 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-700\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:51:05,648 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-700/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:51:05,648 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-700/special_tokens_map.json\n",
            "11/21/2023 20:51:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.6258, 'learning_rate': 4.3837e-05, 'epoch': 0.69}\n",
            "{'loss': 2.6258, 'learning_rate': 4.3836826752299215e-05, 'epoch': 0.69}\n",
            "11/21/2023 20:51:09 - INFO - llmtuner.extras.callbacks - {'loss': 2.1668, 'learning_rate': 4.3753e-05, 'epoch': 0.69}\n",
            "{'loss': 2.1668, 'learning_rate': 4.375294499153139e-05, 'epoch': 0.69}\n",
            "11/21/2023 20:51:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.9746, 'learning_rate': 4.3669e-05, 'epoch': 0.69}\n",
            "{'loss': 2.9746, 'learning_rate': 4.3668577679556916e-05, 'epoch': 0.69}\n",
            "11/21/2023 20:51:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.5621, 'learning_rate': 4.3584e-05, 'epoch': 0.70}\n",
            "{'loss': 2.5621, 'learning_rate': 4.3583727000814034e-05, 'epoch': 0.7}\n",
            "11/21/2023 20:51:14 - INFO - llmtuner.extras.callbacks - {'loss': 2.5328, 'learning_rate': 4.3498e-05, 'epoch': 0.70}\n",
            "{'loss': 2.5328, 'learning_rate': 4.349839515225631e-05, 'epoch': 0.7}\n",
            "11/21/2023 20:51:15 - INFO - llmtuner.extras.callbacks - {'loss': 2.8176, 'learning_rate': 4.3413e-05, 'epoch': 0.71}\n",
            "{'loss': 2.8176, 'learning_rate': 4.3412584343295773e-05, 'epoch': 0.71}\n",
            "11/21/2023 20:51:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.3865, 'learning_rate': 4.3326e-05, 'epoch': 0.71}\n",
            "{'loss': 2.3865, 'learning_rate': 4.332629679574566e-05, 'epoch': 0.71}\n",
            "11/21/2023 20:51:19 - INFO - llmtuner.extras.callbacks - {'loss': 2.2025, 'learning_rate': 4.3240e-05, 'epoch': 0.72}\n",
            "{'loss': 2.2025, 'learning_rate': 4.3239534743762966e-05, 'epoch': 0.72}\n",
            "11/21/2023 20:51:21 - INFO - llmtuner.extras.callbacks - {'loss': 2.4348, 'learning_rate': 4.3152e-05, 'epoch': 0.72}\n",
            "{'loss': 2.4348, 'learning_rate': 4.3152300433790526e-05, 'epoch': 0.72}\n",
            "11/21/2023 20:51:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.3984, 'learning_rate': 4.3065e-05, 'epoch': 0.73}\n",
            "{'loss': 2.3984, 'learning_rate': 4.306459612449888e-05, 'epoch': 0.73}\n",
            "11/21/2023 20:51:24 - INFO - llmtuner.extras.callbacks - {'loss': 2.8186, 'learning_rate': 4.2976e-05, 'epoch': 0.73}\n",
            "{'loss': 2.8186, 'learning_rate': 4.297642408672781e-05, 'epoch': 0.73}\n",
            "11/21/2023 20:51:26 - INFO - llmtuner.extras.callbacks - {'loss': 1.9339, 'learning_rate': 4.2888e-05, 'epoch': 0.74}\n",
            "{'loss': 1.9339, 'learning_rate': 4.288778660342747e-05, 'epoch': 0.74}\n",
            "11/21/2023 20:51:27 - INFO - llmtuner.extras.callbacks - {'loss': 2.2323, 'learning_rate': 4.2799e-05, 'epoch': 0.74}\n",
            "{'loss': 2.2323, 'learning_rate': 4.279868596959941e-05, 'epoch': 0.74}\n",
            "11/21/2023 20:51:29 - INFO - llmtuner.extras.callbacks - {'loss': 2.0736, 'learning_rate': 4.2709e-05, 'epoch': 0.75}\n",
            "{'loss': 2.0736, 'learning_rate': 4.270912449223699e-05, 'epoch': 0.75}\n",
            "11/21/2023 20:51:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.5439, 'learning_rate': 4.2619e-05, 'epoch': 0.75}\n",
            "{'loss': 2.5439, 'learning_rate': 4.2619104490265784e-05, 'epoch': 0.75}\n",
            "11/21/2023 20:51:32 - INFO - llmtuner.extras.callbacks - {'loss': 2.5847, 'learning_rate': 4.2529e-05, 'epoch': 0.76}\n",
            "{'loss': 2.5847, 'learning_rate': 4.252862829448346e-05, 'epoch': 0.76}\n",
            "11/21/2023 20:51:33 - INFO - llmtuner.extras.callbacks - {'loss': 2.3577, 'learning_rate': 4.2438e-05, 'epoch': 0.76}\n",
            "{'loss': 2.3577, 'learning_rate': 4.2437698247499466e-05, 'epoch': 0.76}\n",
            "11/21/2023 20:51:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.5198, 'learning_rate': 4.2346e-05, 'epoch': 0.77}\n",
            "{'loss': 2.5198, 'learning_rate': 4.234631670367437e-05, 'epoch': 0.77}\n",
            "11/21/2023 20:51:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.4113, 'learning_rate': 4.2254e-05, 'epoch': 0.77}\n",
            "{'loss': 2.4113, 'learning_rate': 4.225448602905887e-05, 'epoch': 0.77}\n",
            "11/21/2023 20:51:39 - INFO - llmtuner.extras.callbacks - {'loss': 2.6913, 'learning_rate': 4.2162e-05, 'epoch': 0.78}\n",
            "{'loss': 2.6913, 'learning_rate': 4.216220860133257e-05, 'epoch': 0.78}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:51:39,277 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-800\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:51:39,346 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-800/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:51:39,346 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-800/special_tokens_map.json\n",
            "11/21/2023 20:51:41 - INFO - llmtuner.extras.callbacks - {'loss': 1.7400, 'learning_rate': 4.2069e-05, 'epoch': 0.78}\n",
            "{'loss': 1.74, 'learning_rate': 4.206948680974242e-05, 'epoch': 0.78}\n",
            "11/21/2023 20:51:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.4526, 'learning_rate': 4.1976e-05, 'epoch': 0.79}\n",
            "{'loss': 2.4526, 'learning_rate': 4.197632305504082e-05, 'epoch': 0.79}\n",
            "11/21/2023 20:51:44 - INFO - llmtuner.extras.callbacks - {'loss': 1.9484, 'learning_rate': 4.1883e-05, 'epoch': 0.79}\n",
            "{'loss': 1.9484, 'learning_rate': 4.188271974942347e-05, 'epoch': 0.79}\n",
            "11/21/2023 20:51:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.0869, 'learning_rate': 4.1789e-05, 'epoch': 0.80}\n",
            "{'loss': 2.0869, 'learning_rate': 4.178867931646696e-05, 'epoch': 0.8}\n",
            "11/21/2023 20:51:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.0160, 'learning_rate': 4.1694e-05, 'epoch': 0.80}\n",
            "{'loss': 2.016, 'learning_rate': 4.169420419106593e-05, 'epoch': 0.8}\n",
            "11/21/2023 20:51:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.4654, 'learning_rate': 4.1599e-05, 'epoch': 0.81}\n",
            "{'loss': 2.4654, 'learning_rate': 4.159929681937011e-05, 'epoch': 0.81}\n",
            "11/21/2023 20:51:50 - INFO - llmtuner.extras.callbacks - {'loss': 2.4102, 'learning_rate': 4.1504e-05, 'epoch': 0.81}\n",
            "{'loss': 2.4102, 'learning_rate': 4.150395965872095e-05, 'epoch': 0.81}\n",
            "11/21/2023 20:51:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.4339, 'learning_rate': 4.1408e-05, 'epoch': 0.82}\n",
            "{'loss': 2.4339, 'learning_rate': 4.140819517758795e-05, 'epoch': 0.82}\n",
            "11/21/2023 20:51:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.4447, 'learning_rate': 4.1312e-05, 'epoch': 0.82}\n",
            "{'loss': 2.4447, 'learning_rate': 4.1312005855504876e-05, 'epoch': 0.82}\n",
            "11/21/2023 20:51:56 - INFO - llmtuner.extras.callbacks - {'loss': 2.3315, 'learning_rate': 4.1215e-05, 'epoch': 0.83}\n",
            "{'loss': 2.3315, 'learning_rate': 4.121539418300539e-05, 'epoch': 0.83}\n",
            "11/21/2023 20:51:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.5350, 'learning_rate': 4.1118e-05, 'epoch': 0.83}\n",
            "{'loss': 2.535, 'learning_rate': 4.111836266155868e-05, 'epoch': 0.83}\n",
            "11/21/2023 20:51:59 - INFO - llmtuner.extras.callbacks - {'loss': 2.2915, 'learning_rate': 4.1021e-05, 'epoch': 0.84}\n",
            "{'loss': 2.2915, 'learning_rate': 4.102091380350468e-05, 'epoch': 0.84}\n",
            "11/21/2023 20:52:01 - INFO - llmtuner.extras.callbacks - {'loss': 2.5051, 'learning_rate': 4.0923e-05, 'epoch': 0.84}\n",
            "{'loss': 2.5051, 'learning_rate': 4.0923050131988995e-05, 'epoch': 0.84}\n",
            "11/21/2023 20:52:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.6087, 'learning_rate': 4.0825e-05, 'epoch': 0.85}\n",
            "{'loss': 2.6087, 'learning_rate': 4.082477418089755e-05, 'epoch': 0.85}\n",
            "11/21/2023 20:52:04 - INFO - llmtuner.extras.callbacks - {'loss': 2.6578, 'learning_rate': 4.0726e-05, 'epoch': 0.85}\n",
            "{'loss': 2.6578, 'learning_rate': 4.072608849479106e-05, 'epoch': 0.85}\n",
            "11/21/2023 20:52:05 - INFO - llmtuner.extras.callbacks - {'loss': 2.3923, 'learning_rate': 4.0627e-05, 'epoch': 0.86}\n",
            "{'loss': 2.3923, 'learning_rate': 4.062699562883906e-05, 'epoch': 0.86}\n",
            "11/21/2023 20:52:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.0193, 'learning_rate': 4.0527e-05, 'epoch': 0.86}\n",
            "{'loss': 2.0193, 'learning_rate': 4.0527498148753794e-05, 'epoch': 0.86}\n",
            "11/21/2023 20:52:09 - INFO - llmtuner.extras.callbacks - {'loss': 2.1292, 'learning_rate': 4.0428e-05, 'epoch': 0.86}\n",
            "{'loss': 2.1292, 'learning_rate': 4.0427598630723786e-05, 'epoch': 0.86}\n",
            "11/21/2023 20:52:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.6570, 'learning_rate': 4.0327e-05, 'epoch': 0.87}\n",
            "{'loss': 2.657, 'learning_rate': 4.032729966134711e-05, 'epoch': 0.87}\n",
            "11/21/2023 20:52:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.1115, 'learning_rate': 4.0227e-05, 'epoch': 0.87}\n",
            "{'loss': 2.1115, 'learning_rate': 4.0226603837564434e-05, 'epoch': 0.87}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:52:12,601 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-900\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:52:12,698 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-900/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:52:12,698 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-900/special_tokens_map.json\n",
            "11/21/2023 20:52:14 - INFO - llmtuner.extras.callbacks - {'loss': 1.8632, 'learning_rate': 4.0126e-05, 'epoch': 0.88}\n",
            "{'loss': 1.8632, 'learning_rate': 4.0125513766591796e-05, 'epoch': 0.88}\n",
            "11/21/2023 20:52:16 - INFO - llmtuner.extras.callbacks - {'loss': 1.9702, 'learning_rate': 4.0024e-05, 'epoch': 0.88}\n",
            "{'loss': 1.9702, 'learning_rate': 4.002403206585307e-05, 'epoch': 0.88}\n",
            "11/21/2023 20:52:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.6068, 'learning_rate': 3.9922e-05, 'epoch': 0.89}\n",
            "{'loss': 2.6068, 'learning_rate': 3.9922161362912203e-05, 'epoch': 0.89}\n",
            "11/21/2023 20:52:19 - INFO - llmtuner.extras.callbacks - {'loss': 1.9942, 'learning_rate': 3.9820e-05, 'epoch': 0.89}\n",
            "{'loss': 1.9942, 'learning_rate': 3.98199042954052e-05, 'epoch': 0.89}\n",
            "11/21/2023 20:52:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.2335, 'learning_rate': 3.9717e-05, 'epoch': 0.90}\n",
            "{'loss': 2.2335, 'learning_rate': 3.971726351097179e-05, 'epoch': 0.9}\n",
            "11/21/2023 20:52:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.2876, 'learning_rate': 3.9614e-05, 'epoch': 0.90}\n",
            "{'loss': 2.2876, 'learning_rate': 3.961424166718693e-05, 'epoch': 0.9}\n",
            "11/21/2023 20:52:24 - INFO - llmtuner.extras.callbacks - {'loss': 2.0501, 'learning_rate': 3.9511e-05, 'epoch': 0.91}\n",
            "{'loss': 2.0501, 'learning_rate': 3.951084143149195e-05, 'epoch': 0.91}\n",
            "11/21/2023 20:52:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.4344, 'learning_rate': 3.9407e-05, 'epoch': 0.91}\n",
            "{'loss': 2.4344, 'learning_rate': 3.94070654811255e-05, 'epoch': 0.91}\n",
            "11/21/2023 20:52:27 - INFO - llmtuner.extras.callbacks - {'loss': 2.4841, 'learning_rate': 3.9303e-05, 'epoch': 0.92}\n",
            "{'loss': 2.4841, 'learning_rate': 3.9302916503054246e-05, 'epoch': 0.92}\n",
            "11/21/2023 20:52:29 - INFO - llmtuner.extras.callbacks - {'loss': 2.3036, 'learning_rate': 3.9198e-05, 'epoch': 0.92}\n",
            "{'loss': 2.3036, 'learning_rate': 3.919839719390327e-05, 'epoch': 0.92}\n",
            "11/21/2023 20:52:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.1880, 'learning_rate': 3.9094e-05, 'epoch': 0.93}\n",
            "{'loss': 2.188, 'learning_rate': 3.909351025988627e-05, 'epoch': 0.93}\n",
            "11/21/2023 20:52:32 - INFO - llmtuner.extras.callbacks - {'loss': 2.5079, 'learning_rate': 3.8988e-05, 'epoch': 0.93}\n",
            "{'loss': 2.5079, 'learning_rate': 3.89882584167355e-05, 'epoch': 0.93}\n",
            "11/21/2023 20:52:33 - INFO - llmtuner.extras.callbacks - {'loss': 2.2069, 'learning_rate': 3.8883e-05, 'epoch': 0.94}\n",
            "{'loss': 2.2069, 'learning_rate': 3.8882644389631415e-05, 'epoch': 0.94}\n",
            "11/21/2023 20:52:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.6349, 'learning_rate': 3.8777e-05, 'epoch': 0.94}\n",
            "{'loss': 2.6349, 'learning_rate': 3.8776670913132175e-05, 'epoch': 0.94}\n",
            "11/21/2023 20:52:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.4142, 'learning_rate': 3.8670e-05, 'epoch': 0.95}\n",
            "{'loss': 2.4142, 'learning_rate': 3.8670340731102764e-05, 'epoch': 0.95}\n",
            "11/21/2023 20:52:39 - INFO - llmtuner.extras.callbacks - {'loss': 2.5280, 'learning_rate': 3.8564e-05, 'epoch': 0.95}\n",
            "{'loss': 2.528, 'learning_rate': 3.856365659664399e-05, 'epoch': 0.95}\n",
            "11/21/2023 20:52:40 - INFO - llmtuner.extras.callbacks - {'loss': 1.8418, 'learning_rate': 3.8457e-05, 'epoch': 0.96}\n",
            "{'loss': 1.8418, 'learning_rate': 3.845662127202122e-05, 'epoch': 0.96}\n",
            "11/21/2023 20:52:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.0039, 'learning_rate': 3.8349e-05, 'epoch': 0.96}\n",
            "{'loss': 2.0039, 'learning_rate': 3.834923752859282e-05, 'epoch': 0.96}\n",
            "11/21/2023 20:52:44 - INFO - llmtuner.extras.callbacks - {'loss': 2.0934, 'learning_rate': 3.8242e-05, 'epoch': 0.97}\n",
            "{'loss': 2.0934, 'learning_rate': 3.824150814673841e-05, 'epoch': 0.97}\n",
            "11/21/2023 20:52:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.7533, 'learning_rate': 3.8133e-05, 'epoch': 0.97}\n",
            "{'loss': 2.7533, 'learning_rate': 3.813343591578689e-05, 'epoch': 0.97}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:52:45,836 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-1000\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:52:45,912 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1000/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:52:45,912 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1000/special_tokens_map.json\n",
            "11/21/2023 20:52:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.4737, 'learning_rate': 3.8025e-05, 'epoch': 0.98}\n",
            "{'loss': 2.4737, 'learning_rate': 3.802502363394421e-05, 'epoch': 0.98}\n",
            "11/21/2023 20:52:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.6885, 'learning_rate': 3.7916e-05, 'epoch': 0.98}\n",
            "{'loss': 2.6885, 'learning_rate': 3.7916274108220904e-05, 'epoch': 0.98}\n",
            "11/21/2023 20:52:51 - INFO - llmtuner.extras.callbacks - {'loss': 2.1307, 'learning_rate': 3.7807e-05, 'epoch': 0.99}\n",
            "{'loss': 2.1307, 'learning_rate': 3.780719015435943e-05, 'epoch': 0.99}\n",
            "11/21/2023 20:52:52 - INFO - llmtuner.extras.callbacks - {'loss': 1.6895, 'learning_rate': 3.7698e-05, 'epoch': 0.99}\n",
            "{'loss': 1.6895, 'learning_rate': 3.769777459676126e-05, 'epoch': 0.99}\n",
            "11/21/2023 20:52:54 - INFO - llmtuner.extras.callbacks - {'loss': 3.0061, 'learning_rate': 3.7588e-05, 'epoch': 1.00}\n",
            "{'loss': 3.0061, 'learning_rate': 3.758803026841375e-05, 'epoch': 1.0}\n",
            "11/21/2023 20:52:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.3192, 'learning_rate': 3.7478e-05, 'epoch': 1.00}\n",
            "{'loss': 2.3192, 'learning_rate': 3.7477960010816794e-05, 'epoch': 1.0}\n",
            "11/21/2023 20:52:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.1119, 'learning_rate': 3.7368e-05, 'epoch': 1.01}\n",
            "{'loss': 2.1119, 'learning_rate': 3.7367566673909216e-05, 'epoch': 1.01}\n",
            "11/21/2023 20:52:58 - INFO - llmtuner.extras.callbacks - {'loss': 2.4623, 'learning_rate': 3.7257e-05, 'epoch': 1.01}\n",
            "{'loss': 2.4623, 'learning_rate': 3.725685311599505e-05, 'epoch': 1.01}\n",
            "11/21/2023 20:53:00 - INFO - llmtuner.extras.callbacks - {'loss': 2.8280, 'learning_rate': 3.7146e-05, 'epoch': 1.02}\n",
            "{'loss': 2.828, 'learning_rate': 3.7145822203669465e-05, 'epoch': 1.02}\n",
            "11/21/2023 20:53:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.1971, 'learning_rate': 3.7034e-05, 'epoch': 1.02}\n",
            "{'loss': 2.1971, 'learning_rate': 3.703447681174458e-05, 'epoch': 1.02}\n",
            "11/21/2023 20:53:03 - INFO - llmtuner.extras.callbacks - {'loss': 2.0470, 'learning_rate': 3.6923e-05, 'epoch': 1.03}\n",
            "{'loss': 2.047, 'learning_rate': 3.6922819823175e-05, 'epoch': 1.03}\n",
            "11/21/2023 20:53:05 - INFO - llmtuner.extras.callbacks - {'loss': 1.8347, 'learning_rate': 3.6811e-05, 'epoch': 1.03}\n",
            "{'loss': 1.8347, 'learning_rate': 3.681085412898322e-05, 'epoch': 1.03}\n",
            "11/21/2023 20:53:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.7918, 'learning_rate': 3.6699e-05, 'epoch': 1.03}\n",
            "{'loss': 2.7918, 'learning_rate': 3.669858262818472e-05, 'epoch': 1.03}\n",
            "11/21/2023 20:53:09 - INFO - llmtuner.extras.callbacks - {'loss': 2.2723, 'learning_rate': 3.6586e-05, 'epoch': 1.04}\n",
            "{'loss': 2.2723, 'learning_rate': 3.658600822771292e-05, 'epoch': 1.04}\n",
            "11/21/2023 20:53:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.4711, 'learning_rate': 3.6473e-05, 'epoch': 1.04}\n",
            "{'loss': 2.4711, 'learning_rate': 3.647313384234394e-05, 'epoch': 1.04}\n",
            "11/21/2023 20:53:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.3017, 'learning_rate': 3.6360e-05, 'epoch': 1.05}\n",
            "{'loss': 2.3017, 'learning_rate': 3.63599623946211e-05, 'epoch': 1.05}\n",
            "11/21/2023 20:53:13 - INFO - llmtuner.extras.callbacks - {'loss': 2.5043, 'learning_rate': 3.6246e-05, 'epoch': 1.05}\n",
            "{'loss': 2.5043, 'learning_rate': 3.624649681477923e-05, 'epoch': 1.05}\n",
            "11/21/2023 20:53:15 - INFO - llmtuner.extras.callbacks - {'loss': 2.0627, 'learning_rate': 3.6133e-05, 'epoch': 1.06}\n",
            "{'loss': 2.0627, 'learning_rate': 3.613274004066887e-05, 'epoch': 1.06}\n",
            "11/21/2023 20:53:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.4718, 'learning_rate': 3.6019e-05, 'epoch': 1.06}\n",
            "{'loss': 2.4718, 'learning_rate': 3.6018695017680126e-05, 'epoch': 1.06}\n",
            "11/21/2023 20:53:18 - INFO - llmtuner.extras.callbacks - {'loss': 2.0264, 'learning_rate': 3.5904e-05, 'epoch': 1.07}\n",
            "{'loss': 2.0264, 'learning_rate': 3.5904364698666476e-05, 'epoch': 1.07}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:53:18,954 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-1100\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:53:19,020 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1100/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:53:19,020 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1100/special_tokens_map.json\n",
            "11/21/2023 20:53:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.3911, 'learning_rate': 3.5790e-05, 'epoch': 1.07}\n",
            "{'loss': 2.3911, 'learning_rate': 3.578975204386825e-05, 'epoch': 1.07}\n",
            "11/21/2023 20:53:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.4090, 'learning_rate': 3.5675e-05, 'epoch': 1.08}\n",
            "{'loss': 2.409, 'learning_rate': 3.5674860020836045e-05, 'epoch': 1.08}\n",
            "11/21/2023 20:53:24 - INFO - llmtuner.extras.callbacks - {'loss': 2.1675, 'learning_rate': 3.5560e-05, 'epoch': 1.08}\n",
            "{'loss': 2.1675, 'learning_rate': 3.555969160435383e-05, 'epoch': 1.08}\n",
            "11/21/2023 20:53:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.0948, 'learning_rate': 3.5444e-05, 'epoch': 1.09}\n",
            "{'loss': 2.0948, 'learning_rate': 3.544424977636198e-05, 'epoch': 1.09}\n",
            "11/21/2023 20:53:27 - INFO - llmtuner.extras.callbacks - {'loss': 2.2640, 'learning_rate': 3.5329e-05, 'epoch': 1.09}\n",
            "{'loss': 2.264, 'learning_rate': 3.532853752588002e-05, 'epoch': 1.09}\n",
            "11/21/2023 20:53:29 - INFO - llmtuner.extras.callbacks - {'loss': 2.4056, 'learning_rate': 3.5213e-05, 'epoch': 1.10}\n",
            "{'loss': 2.4056, 'learning_rate': 3.521255784892926e-05, 'epoch': 1.1}\n",
            "11/21/2023 20:53:30 - INFO - llmtuner.extras.callbacks - {'loss': 2.1492, 'learning_rate': 3.5096e-05, 'epoch': 1.10}\n",
            "{'loss': 2.1492, 'learning_rate': 3.509631374845523e-05, 'epoch': 1.1}\n",
            "11/21/2023 20:53:32 - INFO - llmtuner.extras.callbacks - {'loss': 1.9808, 'learning_rate': 3.4980e-05, 'epoch': 1.11}\n",
            "{'loss': 1.9808, 'learning_rate': 3.497980823424989e-05, 'epoch': 1.11}\n",
            "11/21/2023 20:53:34 - INFO - llmtuner.extras.callbacks - {'loss': 2.4952, 'learning_rate': 3.4863e-05, 'epoch': 1.11}\n",
            "{'loss': 2.4952, 'learning_rate': 3.4863044322873735e-05, 'epoch': 1.11}\n",
            "11/21/2023 20:53:35 - INFO - llmtuner.extras.callbacks - {'loss': 2.6580, 'learning_rate': 3.4746e-05, 'epoch': 1.12}\n",
            "{'loss': 2.658, 'learning_rate': 3.474602503757768e-05, 'epoch': 1.12}\n",
            "11/21/2023 20:53:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.5262, 'learning_rate': 3.4629e-05, 'epoch': 1.12}\n",
            "{'loss': 2.5262, 'learning_rate': 3.4628753408224765e-05, 'epoch': 1.12}\n",
            "11/21/2023 20:53:39 - INFO - llmtuner.extras.callbacks - {'loss': 2.2294, 'learning_rate': 3.4511e-05, 'epoch': 1.13}\n",
            "{'loss': 2.2294, 'learning_rate': 3.4511232471211764e-05, 'epoch': 1.13}\n",
            "11/21/2023 20:53:41 - INFO - llmtuner.extras.callbacks - {'loss': 2.6093, 'learning_rate': 3.4393e-05, 'epoch': 1.13}\n",
            "{'loss': 2.6093, 'learning_rate': 3.4393465269390466e-05, 'epoch': 1.13}\n",
            "11/21/2023 20:53:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.1572, 'learning_rate': 3.4275e-05, 'epoch': 1.14}\n",
            "{'loss': 2.1572, 'learning_rate': 3.4275454851989e-05, 'epoch': 1.14}\n",
            "11/21/2023 20:53:44 - INFO - llmtuner.extras.callbacks - {'loss': 1.8967, 'learning_rate': 3.4157e-05, 'epoch': 1.14}\n",
            "{'loss': 1.8967, 'learning_rate': 3.4157204274532794e-05, 'epoch': 1.14}\n",
            "11/21/2023 20:53:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.1852, 'learning_rate': 3.4039e-05, 'epoch': 1.15}\n",
            "{'loss': 2.1852, 'learning_rate': 3.4038716598765524e-05, 'epoch': 1.15}\n",
            "11/21/2023 20:53:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.0373, 'learning_rate': 3.3920e-05, 'epoch': 1.15}\n",
            "{'loss': 2.0373, 'learning_rate': 3.3919994892569795e-05, 'epoch': 1.15}\n",
            "11/21/2023 20:53:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.0164, 'learning_rate': 3.3801e-05, 'epoch': 1.16}\n",
            "{'loss': 2.0164, 'learning_rate': 3.3801042229887756e-05, 'epoch': 1.16}\n",
            "11/21/2023 20:53:50 - INFO - llmtuner.extras.callbacks - {'loss': 1.9151, 'learning_rate': 3.3682e-05, 'epoch': 1.16}\n",
            "{'loss': 1.9151, 'learning_rate': 3.368186169064145e-05, 'epoch': 1.16}\n",
            "11/21/2023 20:53:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.6395, 'learning_rate': 3.3562e-05, 'epoch': 1.17}\n",
            "{'loss': 2.6395, 'learning_rate': 3.356245636065312e-05, 'epoch': 1.17}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:53:52,140 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-1200\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:53:52,206 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1200/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:53:52,207 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1200/special_tokens_map.json\n",
            "11/21/2023 20:53:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.1088, 'learning_rate': 3.3443e-05, 'epoch': 1.17}\n",
            "{'loss': 2.1088, 'learning_rate': 3.344282933156528e-05, 'epoch': 1.17}\n",
            "11/21/2023 20:53:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.2297, 'learning_rate': 3.3323e-05, 'epoch': 1.18}\n",
            "{'loss': 2.2297, 'learning_rate': 3.332298370076068e-05, 'epoch': 1.18}\n",
            "11/21/2023 20:53:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.1324, 'learning_rate': 3.3203e-05, 'epoch': 1.18}\n",
            "{'loss': 2.1324, 'learning_rate': 3.320292257128209e-05, 'epoch': 1.18}\n",
            "11/21/2023 20:53:59 - INFO - llmtuner.extras.callbacks - {'loss': 2.5604, 'learning_rate': 3.3083e-05, 'epoch': 1.19}\n",
            "{'loss': 2.5604, 'learning_rate': 3.3082649051752014e-05, 'epoch': 1.19}\n",
            "11/21/2023 20:54:01 - INFO - llmtuner.extras.callbacks - {'loss': 2.5918, 'learning_rate': 3.2962e-05, 'epoch': 1.19}\n",
            "{'loss': 2.5918, 'learning_rate': 3.2962166256292113e-05, 'epoch': 1.19}\n",
            "11/21/2023 20:54:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.4850, 'learning_rate': 3.2841e-05, 'epoch': 1.20}\n",
            "{'loss': 2.485, 'learning_rate': 3.284147730444263e-05, 'epoch': 1.2}\n",
            "11/21/2023 20:54:04 - INFO - llmtuner.extras.callbacks - {'loss': 2.4023, 'learning_rate': 3.2721e-05, 'epoch': 1.20}\n",
            "{'loss': 2.4023, 'learning_rate': 3.2720585321081654e-05, 'epoch': 1.2}\n",
            "11/21/2023 20:54:05 - INFO - llmtuner.extras.callbacks - {'loss': 2.6971, 'learning_rate': 3.2599e-05, 'epoch': 1.21}\n",
            "{'loss': 2.6971, 'learning_rate': 3.259949343634409e-05, 'epoch': 1.21}\n",
            "11/21/2023 20:54:07 - INFO - llmtuner.extras.callbacks - {'loss': 2.0754, 'learning_rate': 3.2478e-05, 'epoch': 1.21}\n",
            "{'loss': 2.0754, 'learning_rate': 3.247820478554073e-05, 'epoch': 1.21}\n",
            "11/21/2023 20:54:09 - INFO - llmtuner.extras.callbacks - {'loss': 2.6397, 'learning_rate': 3.2357e-05, 'epoch': 1.21}\n",
            "{'loss': 2.6397, 'learning_rate': 3.2356722509077035e-05, 'epoch': 1.21}\n",
            "11/21/2023 20:54:10 - INFO - llmtuner.extras.callbacks - {'loss': 2.5182, 'learning_rate': 3.2235e-05, 'epoch': 1.22}\n",
            "{'loss': 2.5182, 'learning_rate': 3.2235049752371824e-05, 'epoch': 1.22}\n",
            "11/21/2023 20:54:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.2770, 'learning_rate': 3.2113e-05, 'epoch': 1.22}\n",
            "{'loss': 2.277, 'learning_rate': 3.211318966577581e-05, 'epoch': 1.22}\n",
            "11/21/2023 20:54:14 - INFO - llmtuner.extras.callbacks - {'loss': 2.3680, 'learning_rate': 3.1991e-05, 'epoch': 1.23}\n",
            "{'loss': 2.368, 'learning_rate': 3.1991145404490076e-05, 'epoch': 1.23}\n",
            "11/21/2023 20:54:16 - INFO - llmtuner.extras.callbacks - {'loss': 2.3432, 'learning_rate': 3.1869e-05, 'epoch': 1.23}\n",
            "{'loss': 2.3432, 'learning_rate': 3.186892012848432e-05, 'epoch': 1.23}\n",
            "11/21/2023 20:54:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.0886, 'learning_rate': 3.1747e-05, 'epoch': 1.24}\n",
            "{'loss': 2.0886, 'learning_rate': 3.174651700241511e-05, 'epoch': 1.24}\n",
            "11/21/2023 20:54:19 - INFO - llmtuner.extras.callbacks - {'loss': 3.2102, 'learning_rate': 3.1624e-05, 'epoch': 1.24}\n",
            "{'loss': 3.2102, 'learning_rate': 3.162393919554389e-05, 'epoch': 1.24}\n",
            "11/21/2023 20:54:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.4615, 'learning_rate': 3.1501e-05, 'epoch': 1.25}\n",
            "{'loss': 2.4615, 'learning_rate': 3.1501189881654944e-05, 'epoch': 1.25}\n",
            "11/21/2023 20:54:22 - INFO - llmtuner.extras.callbacks - {'loss': 2.3148, 'learning_rate': 3.1378e-05, 'epoch': 1.25}\n",
            "{'loss': 2.3148, 'learning_rate': 3.13782722389732e-05, 'epoch': 1.25}\n",
            "11/21/2023 20:54:24 - INFO - llmtuner.extras.callbacks - {'loss': 1.9836, 'learning_rate': 3.1255e-05, 'epoch': 1.26}\n",
            "{'loss': 1.9836, 'learning_rate': 3.1255189450081977e-05, 'epoch': 1.26}\n",
            "11/21/2023 20:54:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.6709, 'learning_rate': 3.1132e-05, 'epoch': 1.26}\n",
            "{'loss': 2.6709, 'learning_rate': 3.113194470184055e-05, 'epoch': 1.26}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:54:25,903 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-1300\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:54:25,964 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1300/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:54:25,964 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1300/special_tokens_map.json\n",
            "11/21/2023 20:54:27 - INFO - llmtuner.extras.callbacks - {'loss': 2.6102, 'learning_rate': 3.1009e-05, 'epoch': 1.27}\n",
            "{'loss': 2.6102, 'learning_rate': 3.100854118530164e-05, 'epoch': 1.27}\n",
            "11/21/2023 20:54:29 - INFO - llmtuner.extras.callbacks - {'loss': 2.1619, 'learning_rate': 3.0885e-05, 'epoch': 1.27}\n",
            "{'loss': 2.1619, 'learning_rate': 3.0884982095628804e-05, 'epoch': 1.27}\n",
            "11/21/2023 20:54:31 - INFO - llmtuner.extras.callbacks - {'loss': 2.2566, 'learning_rate': 3.0761e-05, 'epoch': 1.28}\n",
            "{'loss': 2.2566, 'learning_rate': 3.0761270632013685e-05, 'epoch': 1.28}\n",
            "11/21/2023 20:54:33 - INFO - llmtuner.extras.callbacks - {'loss': 2.3817, 'learning_rate': 3.0637e-05, 'epoch': 1.28}\n",
            "{'loss': 2.3817, 'learning_rate': 3.063740999759321e-05, 'epoch': 1.28}\n",
            "11/21/2023 20:54:34 - INFO - llmtuner.extras.callbacks - {'loss': 2.5643, 'learning_rate': 3.0513e-05, 'epoch': 1.29}\n",
            "{'loss': 2.5643, 'learning_rate': 3.0513403399366636e-05, 'epoch': 1.29}\n",
            "11/21/2023 20:54:36 - INFO - llmtuner.extras.callbacks - {'loss': 2.1403, 'learning_rate': 3.0389e-05, 'epoch': 1.29}\n",
            "{'loss': 2.1403, 'learning_rate': 3.0389254048112493e-05, 'epoch': 1.29}\n",
            "11/21/2023 20:54:37 - INFO - llmtuner.extras.callbacks - {'loss': 2.1582, 'learning_rate': 3.0265e-05, 'epoch': 1.30}\n",
            "{'loss': 2.1582, 'learning_rate': 3.0264965158305487e-05, 'epoch': 1.3}\n",
            "11/21/2023 20:54:39 - INFO - llmtuner.extras.callbacks - {'loss': 2.4684, 'learning_rate': 3.0141e-05, 'epoch': 1.30}\n",
            "{'loss': 2.4684, 'learning_rate': 3.0140539948033258e-05, 'epoch': 1.3}\n",
            "11/21/2023 20:54:41 - INFO - llmtuner.extras.callbacks - {'loss': 2.3675, 'learning_rate': 3.0016e-05, 'epoch': 1.31}\n",
            "{'loss': 2.3675, 'learning_rate': 3.001598163891305e-05, 'epoch': 1.31}\n",
            "11/21/2023 20:54:42 - INFO - llmtuner.extras.callbacks - {'loss': 2.2229, 'learning_rate': 2.9891e-05, 'epoch': 1.31}\n",
            "{'loss': 2.2229, 'learning_rate': 2.9891293456008302e-05, 'epoch': 1.31}\n",
            "11/21/2023 20:54:44 - INFO - llmtuner.extras.callbacks - {'loss': 2.3988, 'learning_rate': 2.9766e-05, 'epoch': 1.32}\n",
            "{'loss': 2.3988, 'learning_rate': 2.9766478627745147e-05, 'epoch': 1.32}\n",
            "11/21/2023 20:54:45 - INFO - llmtuner.extras.callbacks - {'loss': 2.0034, 'learning_rate': 2.9642e-05, 'epoch': 1.32}\n",
            "{'loss': 2.0034, 'learning_rate': 2.9641540385828813e-05, 'epoch': 1.32}\n",
            "11/21/2023 20:54:47 - INFO - llmtuner.extras.callbacks - {'loss': 2.2154, 'learning_rate': 2.9516e-05, 'epoch': 1.33}\n",
            "{'loss': 2.2154, 'learning_rate': 2.9516481965159975e-05, 'epoch': 1.33}\n",
            "11/21/2023 20:54:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.7011, 'learning_rate': 2.9391e-05, 'epoch': 1.33}\n",
            "{'loss': 2.7011, 'learning_rate': 2.9391306603750947e-05, 'epoch': 1.33}\n",
            "11/21/2023 20:54:50 - INFO - llmtuner.extras.callbacks - {'loss': 2.5285, 'learning_rate': 2.9266e-05, 'epoch': 1.34}\n",
            "{'loss': 2.5285, 'learning_rate': 2.9266017542641894e-05, 'epoch': 1.34}\n",
            "11/21/2023 20:54:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.2992, 'learning_rate': 2.9141e-05, 'epoch': 1.34}\n",
            "{'loss': 2.2992, 'learning_rate': 2.914061802581687e-05, 'epoch': 1.34}\n",
            "11/21/2023 20:54:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.2860, 'learning_rate': 2.9015e-05, 'epoch': 1.35}\n",
            "{'loss': 2.286, 'learning_rate': 2.901511130011988e-05, 'epoch': 1.35}\n",
            "11/21/2023 20:54:55 - INFO - llmtuner.extras.callbacks - {'loss': 2.8129, 'learning_rate': 2.8890e-05, 'epoch': 1.35}\n",
            "{'loss': 2.8129, 'learning_rate': 2.8889500615170755e-05, 'epoch': 1.35}\n",
            "11/21/2023 20:54:57 - INFO - llmtuner.extras.callbacks - {'loss': 2.5171, 'learning_rate': 2.8764e-05, 'epoch': 1.36}\n",
            "{'loss': 2.5171, 'learning_rate': 2.8763789223281057e-05, 'epoch': 1.36}\n",
            "11/21/2023 20:54:59 - INFO - llmtuner.extras.callbacks - {'loss': 2.0231, 'learning_rate': 2.8638e-05, 'epoch': 1.36}\n",
            "{'loss': 2.0231, 'learning_rate': 2.863798037936983e-05, 'epoch': 1.36}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:54:59,414 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-1400\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:54:59,482 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1400/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:54:59,482 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1400/special_tokens_map.json\n",
            "11/21/2023 20:55:01 - INFO - llmtuner.extras.callbacks - {'loss': 2.1682, 'learning_rate': 2.8512e-05, 'epoch': 1.37}\n",
            "{'loss': 2.1682, 'learning_rate': 2.851207734087936e-05, 'epoch': 1.37}\n",
            "11/21/2023 20:55:02 - INFO - llmtuner.extras.callbacks - {'loss': 1.8741, 'learning_rate': 2.8386e-05, 'epoch': 1.37}\n",
            "{'loss': 1.8741, 'learning_rate': 2.838608336769083e-05, 'epoch': 1.37}\n",
            "11/21/2023 20:55:04 - INFO - llmtuner.extras.callbacks - {'loss': 2.8102, 'learning_rate': 2.8260e-05, 'epoch': 1.38}\n",
            "{'loss': 2.8102, 'learning_rate': 2.8260001722039876e-05, 'epoch': 1.38}\n",
            "11/21/2023 20:55:06 - INFO - llmtuner.extras.callbacks - {'loss': 2.5553, 'learning_rate': 2.8134e-05, 'epoch': 1.38}\n",
            "{'loss': 2.5553, 'learning_rate': 2.8133835668432175e-05, 'epoch': 1.38}\n",
            "11/21/2023 20:55:08 - INFO - llmtuner.extras.callbacks - {'loss': 2.6674, 'learning_rate': 2.8008e-05, 'epoch': 1.38}\n",
            "{'loss': 2.6674, 'learning_rate': 2.8007588473558875e-05, 'epoch': 1.38}\n",
            "11/21/2023 20:55:09 - INFO - llmtuner.extras.callbacks - {'loss': 1.9118, 'learning_rate': 2.7881e-05, 'epoch': 1.39}\n",
            "{'loss': 1.9118, 'learning_rate': 2.788126340621205e-05, 'epoch': 1.39}\n",
            "11/21/2023 20:55:11 - INFO - llmtuner.extras.callbacks - {'loss': 2.0423, 'learning_rate': 2.7755e-05, 'epoch': 1.39}\n",
            "{'loss': 2.0423, 'learning_rate': 2.775486373720003e-05, 'epoch': 1.39}\n",
            "11/21/2023 20:55:12 - INFO - llmtuner.extras.callbacks - {'loss': 2.4741, 'learning_rate': 2.7628e-05, 'epoch': 1.40}\n",
            "{'loss': 2.4741, 'learning_rate': 2.7628392739262753e-05, 'epoch': 1.4}\n",
            "11/21/2023 20:55:14 - INFO - llmtuner.extras.callbacks - {'loss': 2.1013, 'learning_rate': 2.7502e-05, 'epoch': 1.40}\n",
            "{'loss': 2.1013, 'learning_rate': 2.750185368698699e-05, 'epoch': 1.4}\n",
            "11/21/2023 20:55:16 - INFO - llmtuner.extras.callbacks - {'loss': 2.5657, 'learning_rate': 2.7375e-05, 'epoch': 1.41}\n",
            "{'loss': 2.5657, 'learning_rate': 2.7375249856721573e-05, 'epoch': 1.41}\n",
            "11/21/2023 20:55:17 - INFO - llmtuner.extras.callbacks - {'loss': 2.2106, 'learning_rate': 2.7249e-05, 'epoch': 1.41}\n",
            "{'loss': 2.2106, 'learning_rate': 2.724858452649258e-05, 'epoch': 1.41}\n",
            "11/21/2023 20:55:20 - INFO - llmtuner.extras.callbacks - {'loss': 2.5836, 'learning_rate': 2.7122e-05, 'epoch': 1.42}\n",
            "{'loss': 2.5836, 'learning_rate': 2.712186097591843e-05, 'epoch': 1.42}\n",
            "11/21/2023 20:55:21 - INFO - llmtuner.extras.callbacks - {'loss': 2.4485, 'learning_rate': 2.6995e-05, 'epoch': 1.42}\n",
            "{'loss': 2.4485, 'learning_rate': 2.6995082486124984e-05, 'epoch': 1.42}\n",
            "11/21/2023 20:55:23 - INFO - llmtuner.extras.callbacks - {'loss': 2.6021, 'learning_rate': 2.6868e-05, 'epoch': 1.43}\n",
            "{'loss': 2.6021, 'learning_rate': 2.686825233966061e-05, 'epoch': 1.43}\n",
            "11/21/2023 20:55:25 - INFO - llmtuner.extras.callbacks - {'loss': 2.6021, 'learning_rate': 2.6741e-05, 'epoch': 1.43}\n",
            "{'loss': 2.6021, 'learning_rate': 2.6741373820411143e-05, 'epoch': 1.43}\n",
            "11/21/2023 20:55:26 - INFO - llmtuner.extras.callbacks - {'loss': 1.8195, 'learning_rate': 2.6614e-05, 'epoch': 1.44}\n",
            "{'loss': 1.8195, 'learning_rate': 2.661445021351491e-05, 'epoch': 1.44}\n",
            "11/21/2023 20:55:28 - INFO - llmtuner.extras.callbacks - {'loss': 1.9603, 'learning_rate': 2.6487e-05, 'epoch': 1.44}\n",
            "{'loss': 1.9603, 'learning_rate': 2.6487484805277624e-05, 'epoch': 1.44}\n",
            "11/21/2023 20:55:29 - INFO - llmtuner.extras.callbacks - {'loss': 1.9272, 'learning_rate': 2.6360e-05, 'epoch': 1.45}\n",
            "{'loss': 1.9272, 'learning_rate': 2.636048088308733e-05, 'epoch': 1.45}\n",
            "11/21/2023 20:55:31 - INFO - llmtuner.extras.callbacks - {'loss': 2.3253, 'learning_rate': 2.6233e-05, 'epoch': 1.45}\n",
            "{'loss': 2.3253, 'learning_rate': 2.623344173532929e-05, 'epoch': 1.45}\n",
            "11/21/2023 20:55:32 - INFO - llmtuner.extras.callbacks - {'loss': 1.9545, 'learning_rate': 2.6106e-05, 'epoch': 1.46}\n",
            "{'loss': 1.9545, 'learning_rate': 2.61063706513008e-05, 'epoch': 1.46}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:55:32,912 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam/checkpoint-1500\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:55:32,977 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1500/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:55:32,977 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/checkpoint-1500/special_tokens_map.json\n",
            "11/21/2023 20:55:34 - INFO - llmtuner.extras.callbacks - {'loss': 2.3708, 'learning_rate': 2.5979e-05, 'epoch': 1.46}\n",
            "{'loss': 2.3708, 'learning_rate': 2.597927092112607e-05, 'epoch': 1.46}\n",
            "11/21/2023 20:55:36 - INFO - llmtuner.extras.callbacks - {'loss': 2.4655, 'learning_rate': 2.5852e-05, 'epoch': 1.47}\n",
            "{'loss': 2.4655, 'learning_rate': 2.5852145835671e-05, 'epoch': 1.47}\n",
            "11/21/2023 20:55:38 - INFO - llmtuner.extras.callbacks - {'loss': 2.6129, 'learning_rate': 2.5725e-05, 'epoch': 1.47}\n",
            "{'loss': 2.6129, 'learning_rate': 2.5724998686457997e-05, 'epoch': 1.47}\n",
            "11/21/2023 20:55:40 - INFO - llmtuner.extras.callbacks - {'loss': 2.1926, 'learning_rate': 2.5598e-05, 'epoch': 1.48}\n",
            "{'loss': 2.1926, 'learning_rate': 2.5597832765580753e-05, 'epoch': 1.48}\n",
            "11/21/2023 20:55:41 - INFO - llmtuner.extras.callbacks - {'loss': 1.7891, 'learning_rate': 2.5471e-05, 'epoch': 1.48}\n",
            "{'loss': 1.7891, 'learning_rate': 2.5470651365618976e-05, 'epoch': 1.48}\n",
            "11/21/2023 20:55:43 - INFO - llmtuner.extras.callbacks - {'loss': 2.6907, 'learning_rate': 2.5343e-05, 'epoch': 1.49}\n",
            "{'loss': 2.6907, 'learning_rate': 2.5343457779553164e-05, 'epoch': 1.49}\n",
            "11/21/2023 20:55:44 - INFO - llmtuner.extras.callbacks - {'loss': 2.7212, 'learning_rate': 2.5216e-05, 'epoch': 1.49}\n",
            "{'loss': 2.7212, 'learning_rate': 2.5216255300679364e-05, 'epoch': 1.49}\n",
            "11/21/2023 20:55:46 - INFO - llmtuner.extras.callbacks - {'loss': 2.4210, 'learning_rate': 2.5089e-05, 'epoch': 1.50}\n",
            "{'loss': 2.421, 'learning_rate': 2.5089047222523838e-05, 'epoch': 1.5}\n",
            "11/21/2023 20:55:47 - INFO - llmtuner.extras.callbacks - {'loss': 1.9440, 'learning_rate': 2.4962e-05, 'epoch': 1.50}\n",
            "{'loss': 1.944, 'learning_rate': 2.496183683875783e-05, 'epoch': 1.5}\n",
            "11/21/2023 20:55:49 - INFO - llmtuner.extras.callbacks - {'loss': 2.6433, 'learning_rate': 2.4835e-05, 'epoch': 1.51}\n",
            "{'loss': 2.6433, 'learning_rate': 2.4834627443112305e-05, 'epoch': 1.51}\n",
            "11/21/2023 20:55:51 - INFO - llmtuner.extras.callbacks - {'loss': 2.0659, 'learning_rate': 2.4707e-05, 'epoch': 1.51}\n",
            "{'loss': 2.0659, 'learning_rate': 2.470742232929263e-05, 'epoch': 1.51}\n",
            "11/21/2023 20:55:52 - INFO - llmtuner.extras.callbacks - {'loss': 2.5016, 'learning_rate': 2.4580e-05, 'epoch': 1.52}\n",
            "{'loss': 2.5016, 'learning_rate': 2.45802247908933e-05, 'epoch': 1.52}\n",
            "11/21/2023 20:55:54 - INFO - llmtuner.extras.callbacks - {'loss': 2.4161, 'learning_rate': 2.4453e-05, 'epoch': 1.52}\n",
            "{'loss': 2.4161, 'learning_rate': 2.4453038121312667e-05, 'epoch': 1.52}\n",
            "11/21/2023 20:55:56 - INFO - llmtuner.extras.callbacks - {'loss': 2.6195, 'learning_rate': 2.4326e-05, 'epoch': 1.53}\n",
            "{'loss': 2.6195, 'learning_rate': 2.432586561366769e-05, 'epoch': 1.53}\n",
            "11/21/2023 20:55:58 - INFO - llmtuner.extras.callbacks - {'loss': 1.9962, 'learning_rate': 2.4199e-05, 'epoch': 1.53}\n",
            "{'loss': 1.9962, 'learning_rate': 2.419871056070862e-05, 'epoch': 1.53}\n",
            "11/21/2023 20:55:59 - INFO - llmtuner.extras.callbacks - {'loss': 2.4193, 'learning_rate': 2.4072e-05, 'epoch': 1.54}\n",
            "{'loss': 2.4193, 'learning_rate': 2.4071576254733794e-05, 'epoch': 1.54}\n",
            "11/21/2023 20:56:01 - INFO - llmtuner.extras.callbacks - {'loss': 1.8823, 'learning_rate': 2.3944e-05, 'epoch': 1.54}\n",
            "{'loss': 1.8823, 'learning_rate': 2.3944465987504348e-05, 'epoch': 1.54}\n",
            "11/21/2023 20:56:02 - INFO - llmtuner.extras.callbacks - {'loss': 2.4858, 'learning_rate': 2.3817e-05, 'epoch': 1.55}\n",
            "{'loss': 2.4858, 'learning_rate': 2.3817383050159008e-05, 'epoch': 1.55}\n",
            "[INFO|trainer.py:2017] 2023-11-21 20:56:03,930 >> \n",
            "\n",
            "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
            "\n",
            "\n",
            "11/21/2023 20:56:03 - INFO - llmtuner.extras.callbacks - {'loss': 0.0000, 'learning_rate': 0.0000e+00, 'epoch': 1.55}\n",
            "{'train_runtime': 529.3591, 'train_samples_per_second': 5.832, 'train_steps_per_second': 5.832, 'train_loss': 2.3742041668649447, 'epoch': 1.55}\n",
            "[INFO|trainer.py:2939] 2023-11-21 20:56:03,932 >> Saving model checkpoint to saves/Phi1.5-1.3B/lora/sam\n",
            "[INFO|tokenization_utils_base.py:2420] 2023-11-21 20:56:03,975 >> tokenizer config file saved in saves/Phi1.5-1.3B/lora/sam/tokenizer_config.json\n",
            "[INFO|tokenization_utils_base.py:2429] 2023-11-21 20:56:03,976 >> Special tokens file saved in saves/Phi1.5-1.3B/lora/sam/special_tokens_map.json\n",
            "***** train metrics *****\n",
            "  epoch                    =       1.55\n",
            "  train_loss               =     2.3742\n",
            "  train_runtime            = 0:08:49.35\n",
            "  train_samples_per_second =      5.832\n",
            "  train_steps_per_second   =      5.832\n",
            "[INFO|modelcard.py:452] 2023-11-21 20:56:04,031 >> Dropping the following result as it does not have all the necessary fields:\n",
            "{'task': {'name': 'Causal Language Modeling', 'type': 'text-generation'}}\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 0.0.0.0:7860 <> https://d3f5150774e76a86ac.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python src/web_demo.py \\\n",
        "    --model_name_or_path microsoft/phi-1_5 \\\n",
        "    --template alpaca \\\n",
        "    --finetuning_type lora \\\n",
        "    --checkpoint_dir /content/LLaMA-Factory/saves/Phi1.5-1.3B/lora/sam/checkpoint-1500"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eXy47A3rMo3",
        "outputId": "3ee1f27a-18c8-4fa3-975a-0d29d6d1748e"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-11-21 21:00:54.484381: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "2023-11-21 21:00:54.484445: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "2023-11-21 21:00:54.484480: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2023-11-21 21:00:56.105306: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "/usr/local/lib/python3.10/dist-packages/trl/trainer/ppo_config.py:141: UserWarning: The `optimize_cuda_cache` arguement will be deprecated soon, please use `optimize_device_cache` instead.\n",
            "  warnings.warn(\n",
            "11/21/2023 21:01:04 - INFO - llmtuner.model.adapter - Fine-tuning method: LoRA\n",
            "11/21/2023 21:01:12 - INFO - llmtuner.model.adapter - Merged 1 model checkpoint(s).\n",
            "11/21/2023 21:01:12 - INFO - llmtuner.model.adapter - Loaded fine-tuned model from checkpoint(s): /content/LLaMA-Factory/saves/Phi1.5-1.3B/lora/sam/checkpoint-1500\n",
            "11/21/2023 21:01:12 - INFO - llmtuner.model.loader - trainable params: 0 || all params: 1418270720 || trainable%: 0.0000\n",
            "11/21/2023 21:01:12 - INFO - llmtuner.model.loader - This IS expected that the trainable params is 0 if you are using model for inference only.\n",
            "11/21/2023 21:01:13 - INFO - llmtuner.data.template - Add pad token: <|endoftext|>\n",
            "Running on local URL:  http://0.0.0.0:7860\n",
            "Running on public URL: https://c5f14af6120aa4130d.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2361, in block_thread\n",
            "    time.sleep(0.1)\n",
            "KeyboardInterrupt\n",
            "\n",
            "During handling of the above exception, another exception occurred:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/LLaMA-Factory/src/web_demo.py\", line 11, in <module>\n",
            "    main()\n",
            "  File \"/content/LLaMA-Factory/src/web_demo.py\", line 7, in main\n",
            "    demo.launch(server_name=\"0.0.0.0\", server_port=7860, share=True, inbrowser=True)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2266, in launch\n",
            "    self.block_thread()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/blocks.py\", line 2365, in block_thread\n",
            "    self.server.close()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gradio/networking.py\", line 75, in close\n",
            "    self.thread.join()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1096, in join\n",
            "    self._wait_for_tstate_lock()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1116, in _wait_for_tstate_lock\n",
            "    if lock.acquire(block, timeout):\n",
            "KeyboardInterrupt\n",
            "Killing tunnel 0.0.0.0:7860 <> https://c5f14af6120aa4130d.gradio.live\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPeXNZxNvjVv",
        "outputId": "386f0038-7e66-4cbf-fca7-450f714b1d6c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-11-21 21:17:05--  https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://github.com/cloudflare/cloudflared/releases/download/2023.10.0/cloudflared-linux-amd64 [following]\n",
            "--2023-11-21 21:17:05--  https://github.com/cloudflare/cloudflared/releases/download/2023.10.0/cloudflared-linux-amd64\n",
            "Reusing existing connection to github.com:443.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/40a2d641-06b2-41f5-baf3-3813ba09a2e8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231121%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231121T211705Z&X-Amz-Expires=300&X-Amz-Signature=0e61d4e6ac81e6241f5d1f8057984909faf9f927ef075a485c0fb6a605fb90ee&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream [following]\n",
            "--2023-11-21 21:17:05--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/106867604/40a2d641-06b2-41f5-baf3-3813ba09a2e8?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=AKIAIWNJYAX4CSVEH53A%2F20231121%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20231121T211705Z&X-Amz-Expires=300&X-Amz-Signature=0e61d4e6ac81e6241f5d1f8057984909faf9f927ef075a485c0fb6a605fb90ee&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=106867604&response-content-disposition=attachment%3B%20filename%3Dcloudflared-linux-amd64&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 36477489 (35M) [application/octet-stream]\n",
            "Saving to: ‚Äòcloudflared‚Äô\n",
            "\n",
            "cloudflared         100%[===================>]  34.79M   177MB/s    in 0.2s    \n",
            "\n",
            "2023-11-21 21:17:06 (177 MB/s) - ‚Äòcloudflared‚Äô saved [36477489/36477489]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5b15kDk6vc3z",
        "outputId": "dd8bba62-80e9-4f9b-e212-0918b4d5f54f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start cloudflared runnel\n",
            "2023-11-21T21:17:25Z INF Requesting new quick Tunnel on trycloudflare.com...\n",
            "2023-11-21T21:17:26Z INF |  https://donation-north-pledge-cord.trycloudflare.com                                      |\n"
          ]
        }
      ]
    }
  ]
}